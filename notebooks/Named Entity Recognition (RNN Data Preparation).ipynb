{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "687c79b0-ad46-404f-9157-a2e2b9401ab4",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a05409-00d1-4069-acb2-dbc32f80c801",
   "metadata": {},
   "source": [
    "# NER - GMB Dataset (Groningen Meaning Bank)\n",
    "\n",
    "The Groningen Meaning Bank (GMB) dataset is a corpus of annotated text for natural language processing (NLP) tasks, including named entity recognition (NER). The corpus contains over 1.5 million words of text from multiple sources, including news articles, Wikipedia articles, and legal documents, and is annotated with various types of linguistic information, including part-of-speech tags, dependency parses, and named entities.\n",
    "\n",
    "The GMB dataset is notable for its high-quality annotations and wide coverage of multiple languages, including English, Dutch, and Spanish. In particular, the English portion of the dataset contains over 500,000 words of annotated text, with named entities annotated for person, organization, and location.\n",
    "\n",
    "The GMB dataset is freely available for research and academic purposes, and has been used in various NLP research projects, including the development of NER models using machine learning and deep learning techniques. The dataset is also used as a benchmark for evaluating the performance of NER models in research papers and competitions. Overall, the GMB dataset is a valuable resource for anyone interested in NLP, and has contributed significantly to the development of NER models and other NLP tasks.\n",
    "\n",
    "In this notebook, we prepare the dataset for training RNN-based models for NER later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c3f20",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61faac-8175-4453-94e1-a1236a5964e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6890691c-46d1-45a8-a6da-c061984c6f41",
   "metadata": {},
   "source": [
    "Lastly, `src/utils.py` provides a utility method to decompress files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f9aed-2e4c-466a-8cf0-be67314f71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import decompress_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4360d38-4ace-4d3a-ae5d-6a7f37547b34",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d93efb",
   "metadata": {},
   "source": [
    "## Load Data from File\n",
    "\n",
    "Dataset we use in this notebook is taken from [Kaggle](https://www.kaggle.com/datasets/naseralqaydeh/named-entity-recognition-ner-corpus) from Kaggle. We provide this dataset here in the repository as a `zip` file, so we first need to extract the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428013c9-308d-42ba-a39e-9136a70facea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decompress file...')\n",
    "decompress_file('data/datasets/gmb-ner/gmb-ner.zip', 'data/datasets/gmb-ner/')\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338ab7c-c9ad-4fb7-97a4-8e289cd3b839",
   "metadata": {},
   "source": [
    "Now we can read the extracted `csv` file as usual using `pandas` and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d787b-d37a-4970-b3ca-9181b34aa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/datasets/gmb-ner/gmb-ner.csv\", sep=\",\", encoding='Latin-1')\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c7b67-c73e-4725-820c-31c0fe157d47",
   "metadata": {},
   "source": [
    "As you can see from the output above each line in the file represents a word that comes with its POS tag and NER labels. So let's first loop over all lines and create sentences as a list of (word, pos, label) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99ad42-d434-4f9a-9cbd-581982a9687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent, sentences = [], []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    nr, word, pos, label = str(row[1]), row[2], row[3], row[4]\n",
    "\n",
    "    # Check if we have reached the next sentence\n",
    "    if 'Sentence' in nr:\n",
    "        # If the current sentence is not empty (just a fail safe) add it to the list of all sentences\n",
    "        if len(sent) > 0:\n",
    "            sentences.append(sent)\n",
    "        sent = []\n",
    "\n",
    "    # Add current word, POS tag, and NER label to the current sentence\n",
    "    if isinstance(word, str) is True:\n",
    "        sent.append((word, pos, label))\n",
    "\n",
    "# Print the number of sentences\n",
    "print(\"Number of sentences: {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce15fb-95b4-4c64-80f4-713861ebf92e",
   "metadata": {},
   "source": [
    "## Create Vocabularies\n",
    "\n",
    "Now we perform the well-known steps of creating the vocabularies and vectorizing each sentence to be used for training neural networks. You can check out previous lecture notebooks and provide more details on the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe1b9a-9142-45ce-96ff-c07e672c4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "pos_counter = Counter()\n",
    "label_counter = Counter()\n",
    "\n",
    "for sent in sentences:\n",
    "    for token, pos, tag in sent:\n",
    "        token_counter[token] += 1\n",
    "        pos_counter[pos] += 1\n",
    "        label_counter[tag] += 1\n",
    "\n",
    "print(len(token_counter))\n",
    "print(len(pos_counter))\n",
    "print(len(label_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9659649-24fd-471c-81a5-6c6b4f650959",
   "metadata": {},
   "source": [
    "Let's have a quick look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021989ec-b4d0-48c2-ab39-fd44aadf6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894dff9-f84d-4911-ba83-1a555f0de3d1",
   "metadata": {},
   "source": [
    "Each element of a sentence/sequence is a 3-tuple containing the word/token, the POS tag and the NER label.\n",
    "\n",
    "The code cell below performs all the steps to create the required vocabularies as we have seen in multiple other notebooks, so we skip a more detailed discussion of each individual step in the code cell. However, note that we have to create a vocabulary for all three components: the words/tokens, the POS tags and the NER label. Considering `SPECIALS` and `UNK_TOKEN` also for the POS tags and the NER labels is probably not needed -- particularly for sufficiently large datasets -- but it doesn't harm either and so we are on the safe side (in case we would indeed encounter an unknown POS tag or NER label).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37d889-22a5-40d6-b40b-d294b273e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "SPECIALS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "\n",
    "## Sort word frequencies and conver to an OrderedDict\n",
    "token_counter_sorted = sorted(token_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "pos_counter_sorted = sorted(pos_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "label_counter_sorted = sorted(label_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "max_words = 9999999999999999 # all words, by default (so just use a very large number)\n",
    "token_ordered_dict = OrderedDict(token_counter_sorted[:max_words])\n",
    "pos_ordered_dict = OrderedDict(pos_counter_sorted)\n",
    "label_ordered_dict = OrderedDict(label_counter_sorted)\n",
    "\n",
    "for t in token_ordered_dict:\n",
    "    if isinstance(t, str) is False:\n",
    "        print(t, type(t))\n",
    "    #break\n",
    "\n",
    "## Create vocabularies\n",
    "vocab_token = vocab(token_ordered_dict, specials=SPECIALS)\n",
    "vocab_pos = vocab(pos_ordered_dict, specials=SPECIALS)\n",
    "vocab_label = vocab(label_ordered_dict, specials=SPECIALS)\n",
    "\n",
    "vocab_token.set_default_index(vocab_token[UNK_TOKEN])\n",
    "vocab_pos.set_default_index(vocab_pos[UNK_TOKEN])\n",
    "vocab_label.set_default_index(vocab_label[UNK_TOKEN])\n",
    "\n",
    "print(\"Size of token vocabulary: {}\".format(len(vocab_token)))\n",
    "print(\"Size of POS vocabulary: {}\".format(len(vocab_pos)))\n",
    "print(\"Size of label vocabulary: {}\".format(len(vocab_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef73533-0801-4602-9154-825a6f071ff9",
   "metadata": {},
   "source": [
    "We need to save all vocabularies for later use when training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213b813-fd09-436b-8380-70b8cb27b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab_token, \"data/datasets/gmb-ner/gmb-ner-token.vocab\")\n",
    "torch.save(vocab_pos, \"data/datasets/gmb-ner/gmb-ner-pos.vocab\")\n",
    "torch.save(vocab_label, \"data/datasets/gmb-ner/gmb-ner-label.vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2f6a2-f532-4046-9924-7bb7f51e41c4",
   "metadata": {},
   "source": [
    "## Vectorize Data\n",
    "\n",
    "In the last step, we vectorize our sentences. Note that the code cell below considers only sentences for length 5..50 which is by far the majority of sentences. This is just for convenience when we train our models. Note also that we simply concatenate the token indices and POS tag indices into a single sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a96fd-ebb6-4e2f-94c7-e0b89a8d9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"data/datasets/gmb-ner/gmb-ner-data-vectorized.txt\", \"w\")\n",
    "\n",
    "min_sent_len, max_sent_len = 5, 50\n",
    "\n",
    "with tqdm(total=len(sentences)) as pbar:\n",
    "    for sent in sentences:\n",
    "        seq_token = [ tup[0] for tup in sent ]\n",
    "        seq_pos = [ tup[1] for tup in sent ]\n",
    "        seq_label = [ tup[2] for tup in sent ]\n",
    "    \n",
    "        vec_token = vocab_token.lookup_indices(seq_token)\n",
    "        vec_pos = vocab_pos.lookup_indices(seq_pos)\n",
    "        vec_label = vocab_label.lookup_indices(seq_label)\n",
    "    \n",
    "        str_token_pos = \" \".join([str(idx) for idx in vec_token+vec_pos])\n",
    "        str_label = \" \".join([str(idx) for idx in vec_label])\n",
    "        output_file.write(\"{},{}\\n\".format(str_token_pos, str_label))\n",
    "    \n",
    "        pbar.update(1)\n",
    "        \n",
    "        \n",
    "output_file.flush()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34452477-4e26-4451-9d38-231de2f4860f",
   "metadata": {},
   "source": [
    "To show an example, the code cell below prints the last vectorized data sample. Keep in mind that the first half of the sentence represents the indices of the word of the sentence (`207 27 42 163 7 4 1446 756 1510 2057 5`), and the second half represents the indices of the corresponding POS tags (`22 21 19 7 6 7 9 11 6 4 10`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409ae446-7cf3-4c48-9176-95aedc0620f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str_token_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fffcdd-e0b8-4cdd-834a-7040b2c555f4",
   "metadata": {},
   "source": [
    "We can also look at the NER labels for each word. Since the last sentence does not contain any named entities, all words are labeled with `O` (Other), which is represented by the index `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4891ff-093f-403e-9d74-c101a515784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140113c4-ea25-45bf-8b70-4e25e4096f24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b265c4c-c24a-43e4-b757-3062d857ab24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The file `gmb-ner-data-vectorized.txt` now contains all sentences and corresponding POS tags in vectorized form -- that is, each word and each POS tag is represented by its unique numerical index (i.e., integer value). This representation of the dataset can now serve as input for the RNN-based architectures for training an NER tagger in the notebook \"Named Entity Recognition (NER)\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f5016-a733-4e5d-b511-78adf761d86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
