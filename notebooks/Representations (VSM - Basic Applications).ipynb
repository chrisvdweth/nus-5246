{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958f453a-dc99-4d3e-aa7a-7f3a5b677e05",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911c7f3-8b6d-4c44-bdca-630cd889e2f5",
   "metadata": {},
   "source": [
    "# Basic Applications\n",
    "\n",
    "With representing documents as vectors together with calculating the similarity between two documents using the cosine similarity, we can already address some basic text mining tasks. In this notebook, we look at Keyword Extraction and Document Search in line with the topics covered in the lecture. Please note that we only sketch basic approaches and ideas here; so don't expect production-ready solutions :).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e50b79-3c9d-407e-ac96-78d26c99f045",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee490a-9fb9-4c6c-a41d-d2ea719187ae",
   "metadata": {},
   "source": [
    "### Required packages\n",
    "\n",
    "Apart from the import parts from numpy and scikit-learn, we also need some packages for visualization. This includes the [`wordcloud`](https://anaconda.org/conda-forge/wordcloud) package to generate nice-looking word clouds. We also need a couple of auxiliary methods provided in `src/utils.py`. \"Outsourcing'' that code keeps the notebook clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784257b7-b384-4775-bbe8-0c0604a41944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from src.utils import get_articles, get_random_article, color_func, get_mask, compute_sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51557d92-360d-4920-b53e-0e19578da573",
   "metadata": {},
   "source": [
    "As usual, we also need spaCy to handle the preprocessing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507e5b8-e6b7-4dd3-b276-5ba9b5052c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fdd336-c285-4012-bad0-1bcddfac51dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543cd68d-4d19-4c56-8d92-cfc37812e1b3",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "The file `data/news-articles-preprocessed.zip` contains a text file with 6k+ news articles collected from The Straits Times around Oct/Nov 2022. The articles are already somewhat preprocessed (punctuation removal, converted to lowercase, line break removal, lemmatization). Each line in the text file represents an individual article.\n",
    "\n",
    "To get the article, the method `get_articles()` reads this zip file and loops through the text file and returns all articles in a list. The method also accepts a `search_term` to filter articles that contain that search term. While not used by default in the following, you can check it out to get different results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33dcf0c-77e3-4924-a797-d18f1aeb1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_articles('data/datasets/news-articles/news-articles-preprocessed.zip')\n",
    "#articles = get_articles('data/datasets/news-articles/news-articles-preprocessed.zip', search_term=\"police\")\n",
    "\n",
    "print(\"Number of articles: {}\".format(len(articles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b04254-9d16-486b-9978-5c6fd6320e8d",
   "metadata": {},
   "source": [
    "There is also a method `get_random_article()` which, to the surprise of no-one, returns a random article from the list of 6k+ articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8075287-29d3-4703-9bcd-5227dad97d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_article = get_random_article('data/datasets/news-articles/news-articles-preprocessed.zip')\n",
    "\n",
    "print(random_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c61011-8944-4987-89d3-70f3be913ade",
   "metadata": {},
   "source": [
    "From the output of the previous code cell, you can kind of see the preprocessing steps that have been performed or not. For example, stopwords have not been removed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689aac58-913c-4294-bafc-111a108a785f",
   "metadata": {},
   "source": [
    "## Keyword Extractions\n",
    "\n",
    "When we want to understand key information from specific documents, we typically turn towards keyword extraction. Keyword extraction is the automated process of extracting the words and phrases that are most relevant to an input text. There are many ways to approach this task as there are often subtle criteria that make a word or phrase relevant.\n",
    "\n",
    "Using TF-IDF weights is a basic but intuitive approach as it considers a word/phrase relevant for text document if\n",
    "\n",
    "* The word/phrase appears frequently in the document\n",
    "\n",
    "* The word/phrase does not appear frequently in many other documents\n",
    "\n",
    "Let's first fetch a random article that we will use throughout this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9485681-2fed-4252-abfe-97b9bb63a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = get_random_article('data/datasets/news-articles/news-articles-preprocessed.zip')\n",
    "\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9debc69b-6417-47fb-8054-64a8bbcdf248",
   "metadata": {},
   "source": [
    "### Baseline: Using Term Frequencies\n",
    "\n",
    "As a baseline, let's consider first only the term frequency to identify keywords. Since we only look at a single article, we can calculate the term frequencies using very basic packages and methods provided by Python. First, we convert our article to a list of words. This is trivial since we already preprocessed our article so that we can split simply by whitespaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730e2c4-cb5d-4b98-810f-8ce55c0b8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = article.split()\n",
    "\n",
    "print(\"Total number of words in the article: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a628f5e-bc10-4674-8501-c9b3cdb1daf8",
   "metadata": {},
   "source": [
    "Now we can use a [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) to compute the number of occurrences of each word. The result is a dictionary with the keys being the words and the values being the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b764e-7b90-45a3-89e0-c7a50c65f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = Counter(words)\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d49a17-bf85-4c04-917f-4c48ecfe870e",
   "metadata": {},
   "source": [
    "We can now plot the result as a work cloud. The [`wordcloud`](https://anaconda.org/conda-forge/wordcloud) package provides a method `generate_from_frequencies()` that directly takes the dictionary we have just created as input. By default, the generated word clouds don't look very nice, so the code cell below makes to extension\n",
    "\n",
    "* We use a mask to enforce an oval shape (by default: rectangle)\n",
    "\n",
    "* We use a function `color_func` that maps the relevance score of a word to a color; here we actually use the same color to ensure that all words are legible; this also means that the relevance of each word is only marked by its font size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a18746-fa44-4efc-9ce7-6ff012b37c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(color_func=color_func, background_color=\"white\", mask=get_mask(), max_words=500,contour_width=0)\n",
    "\n",
    "wc.generate_from_frequencies(word_freqs)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ae8fb-3e43-4ac1-aadb-cc68486f7a88",
   "metadata": {},
   "source": [
    "Since we consider words as relevant that are frequent -- and we didn't perform stopword removal! -- the most relevant keywords are stopwords. Of course, we know that stopwords don't really carry any interesting meaning. For more useful results, we have 2 options\n",
    "\n",
    "* Remove all stopwords and recalculate the term frequencies\n",
    "\n",
    "* Use TF-IDF weights which will \"penalize\" stopwords as they appear in all documents.\n",
    "\n",
    "In practice, the latter option is often preferred, particularly when larger n-grams are considered and stopwords as part of an n-gram might become important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60933d20-d83f-4f77-99f6-d2a9c9aaa5c1",
   "metadata": {},
   "source": [
    "### Calculate TF_IDF Weights\n",
    "\n",
    "We already saw in the other notebook how easy it is to calculate the TF-IDF weights for a given corpus using scitkit-learn. So let's just do this here. The only noticeable difference is that we set the parameter `max_features`. This was not needed in the other notebook since we only worked with a toy dataset and thus a limited vocabulary. Now we deal with real-world data, where it's common to restrict the vocabulary to some maximum size. Recall from the lecture that concept of sparsity, where corpus typically contains many words that occur very infrequently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2cccb-0d91-43de-863e-2f3544af0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf=False, ngram_range = (1, 1), max_features=20000)\n",
    "\n",
    "# Transform documents to tf-idf vectors\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(articles)\n",
    "\n",
    "# Convert to pandas dataframe -- just for a nice visualization\n",
    "df_tfidf = pd.DataFrame(X_tfidf.A.T, columns=[ \"d{}\".format(d+1) for d in range(len(articles)) ])\n",
    "df_tfidf = df_tfidf.set_index(pd.Index(tfidf_vectorizer.get_feature_names_out()))\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb10748-3c7f-4538-b059-e3eec9904997",
   "metadata": {},
   "source": [
    "Now using a real-world corpus the sparsity is also much higher compared to the values we saw using our toy dataset. This should also make it more obvious why sparse matrix representations are used to store and handle these document-term matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7761ff7-33a3-4da0-9301-1a190723f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_sparsity(X_tfidf.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41097d-64e2-48d8-94d1-49a4689be5d3",
   "metadata": {},
   "source": [
    "### Convert Article & Plot Word Cloud\n",
    "\n",
    "We can now use our trained vectorizer to convert our random article to a document vector with TF-IDF weights. Note that the method `transform()` expects a list as input, so we need to wrap our single article into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a228d-6ed7-49f3-929a-bf8d7d201e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tfidf = tfidf_vectorizer.transform([article])\n",
    "\n",
    "article_tfidf = np.asarray(article_tfidf.todense())\n",
    "\n",
    "print(article_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae1ad8-d7ab-45d4-a162-019b09742799",
   "metadata": {},
   "source": [
    "Of course, the resulting document vector will be sparse vector with most entries in the vector being 0\n",
    "\n",
    "For the word cloud, we first need to extract all the words with non-zero TF-IDF weights together with their respective TF-IDF weights. The code cell below accomplishes this, and again creates a dictionary with the words as the keys and the TF-IDF weights as the values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bae0d8-f12f-4ed8-885a-586dd24b699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tfidf = list(tfidf_vectorizer.get_feature_names_out()[np.nonzero(article_tfidf[0])])\n",
    "\n",
    "weights = list(article_tfidf[np.nonzero(article_tfidf)])\n",
    "\n",
    "word_freqs = { w:weights[idx]  for (idx, w) in enumerate(words_tfidf) }\n",
    "\n",
    "#print(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d99c796-0643-4725-9ba4-1232fbd454d6",
   "metadata": {},
   "source": [
    "The weights are, of course, no longer simple integers. However, the [`wordcloud`](https://anaconda.org/conda-forge/wordcloud) package accepts floats as well since the important information stems from the differences in the weights (not in the absolute values of the weights). So let's run the same code to generate a word cloud, only now using the TF-IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa9e24-156a-469f-9bde-81a80407ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(color_func=color_func, background_color=\"white\", max_words=500, mask=get_mask(), contour_width=0)\n",
    "\n",
    "wc.generate_from_frequencies(word_freqs)\n",
    "\n",
    "plt.figure()\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31b5e3-4322-4aae-b733-6474653a14a1",
   "metadata": {},
   "source": [
    "This result is arguably much more useful as TF-IDF basically ignore stopwords but focus on those word that are arguably indicative/informative for a given document (here: news article). The exact word cloud will, of course, depend on the random article as well as the parameter settings for the `TfidfVectorizer` but also for the `WordCloud`. Most importantly, try different n-gram sizes (or ranges of n-gram sizes) and see how it affects the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfb4d52-8e35-44f6-ab62-02f73133dcea",
   "metadata": {},
   "source": [
    "## Document Search & Ranking\n",
    "\n",
    "Finding documents of interest in a large text corpus is a very important task. The corpus can be the set of websites on the Internet, where finding relevant documents translates to an online search. Most basically, document search assumes an input query containing a set of search terms, and then finding the most relevant documents w.r.t. to these search terms.\n",
    "\n",
    "We saw in the lecture that writing a good query is actually not a trivial task. If we require that all search terms must be included in a document, we might miss out on good results because we included a \"weird\" search term. However, if we include all documents that contain any of the search terms, we are likely to get many documents not relevant w.r.t to the whole query.\n",
    "\n",
    "In the following, we replicate the basic 2-step approach from the lecture:\n",
    "\n",
    "* Fetch all candidate documents (i.e., documents that contain at least on of the search terms)\n",
    "\n",
    "* Rank all candidate terms based on the similarity to the query to identify the most relevant documents.\n",
    "\n",
    "Of course, search engines such as Google or Bing perform way more sophisticated steps even outside methods such as PageRank. But this is outside our scope here and is a major topic in modules such as CS3245 Information Retrieval.\n",
    "\n",
    "Just in case you tried different parameter settings above, let's first recalculate the TF-IDF weights for our corpus of news articles using the default parameter settings used in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76467b83-bea8-4c8b-a928-85b32447bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf=False, ngram_range = (1, 1), max_features=20000)\n",
    "\n",
    "# Transform documents to tf-idf vectors\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd0da9-336d-49d3-9168-e32b82543320",
   "metadata": {},
   "source": [
    "### Find Candidate Documents\n",
    "\n",
    "Since we identify candidate documents by checking if they contain at least one of the search terms -- again, in practice, more sophisticated approaches are performed -- we first need to bring our search terms into the same shape as our articles. For example, since we lemmatized the word in the news articles, we also need to lemmatize the words in our query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551b235-21df-4dee-bcf8-a2924a76be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(query):\n",
    "    # Split query and do some very basic preprocessing (lemmatize, keep only words)\n",
    "    # The preprocessing of the query should match the preprocessing of the documents\n",
    "    return [ t.lemma_.lower() for t in nlp(query) if t.is_alpha==True and t.is_stop==False ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8d527-5ed1-448b-b951-2b3da2921d81",
   "metadata": {},
   "source": [
    "Let's assume the simple query *\"money SCAM victims\"*. Based on our preprocessing steps, we need to lowercase *\"SCAM\"* to *\"scam\"* and lemmatize *\"victims\"* to *\"victim\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1a47d-b5fa-4724-b865-aece75116f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = extract_keywords('money SCAM victims')\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651206d-2084-483d-80c1-792e1c4cfb48",
   "metadata": {},
   "source": [
    "Now we can implement a method `search()` that loops over all articles to check if an article contains at least one of the search terms from our query. While this is OK for just 6k+ short articles, looping over all documents would be impractical on large corpora. In practice, we would create secondary indices that map from a word to all indices of documents containing that word. But again, this is covered in Information Retrieval. Here, we allow ourselves to be naive :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d84c9-db43-4502-8c56-034914778051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(docs, keywords):\n",
    "    result_indices = []\n",
    "    # Loop over each document and check if it should be part of the result\n",
    "    # (NOTE: This is a very naive way to do in practice!)\n",
    "    for idx, doc in enumerate(docs):\n",
    "        # Keep it simple: return documents that contain ANY of the keywords\n",
    "        if any([w in doc.split() for w in keywords]) == True:\n",
    "            result_indices.append(idx)\n",
    "    # We return the indices of the result documents not the documents themselves\n",
    "    return np.asarray(result_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb7d35-1d34-4b82-bd6a-5cb1455185e8",
   "metadata": {},
   "source": [
    "Let's execute the method to find all the candidate articles for our set of query terms. Note that we only return the indices of the articles in the `documents list`. This is sufficient, and even more convenient, since we find the respective rows in our document-term matrix using these indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4abfa-828d-46ec-81a2-1cc8d7bd5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_indices = search(articles, keywords)\n",
    "\n",
    "print(\"Number of cadidate documents: {}\".format(len(result_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2e4c5-bdeb-4618-b753-bd346682e6d2",
   "metadata": {},
   "source": [
    "### Rank Candidate Documents\n",
    "\n",
    "To rank the candidate documents, we need to calculate the cosine similarity between the query and all candidates. While we get the document vectors directly from the document-term matrix, we still need to \"vectorize\" our query. Of course, this we can directly do by using the `transform()` method of the vectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d3f30-36e7-4d54-941f-44a00c003f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tfidf = tfidf_vectorizer.transform([' '.join(keywords)])\n",
    "\n",
    "print(query_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b20729-4e1f-4c75-9a72-a1e663eab278",
   "metadata": {},
   "source": [
    "The methods `rank_results()` now performs the final required steps (a) to compute all cosine similarities between the query, (b) to sort them w.r.t. the resulting similarity values, and (c) to return the top-k candidates as specified by the input parameters `topk`. For this method, we make clever use of method such as [`sklearn.metrics.pairwise.cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) and [`np.argpartition`](https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html) to make our lives easy. Not only is the code much simpler, but the performance is also likely to be much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f4e3f-945c-4c56-931b-77ad0ff634b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_results(result_indices, query_tfidf, X_tfidf, topk=10):\n",
    "    results = []\n",
    "    # Get all tf-idf vectors of the query result candidates\n",
    "    docs_tfidf = X_tfidf.A[result_indices]\n",
    "    # Compute cosine similarities between query and all candidates\n",
    "    cosine_similarities = cosine_similarity(docs_tfidf, query_tfidf).squeeze()\n",
    "    # Consider onlt the top-k cadidates\n",
    "    top_result_indices = np.argpartition(cosine_similarities, -topk)[-topk:]\n",
    "    # We have to return the indices of the documents\n",
    "    return result_indices[top_result_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de77500-8f44-45d7-8c42-8ec082451077",
   "metadata": {},
   "source": [
    "Now we can call the method `rank_results()` to give us the `topk` most relevant articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b004a-b82d-41ce-8cad-86b49784a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in rank_results(result_indices, query_tfidf, X_tfidf, topk=3):\n",
    "    print(articles[index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cba7fa-5b64-4035-9bd5-5bcf6131dd16",
   "metadata": {},
   "source": [
    "Of course, in a practial system, we would new return the preprocessed string of the article but IDs or links to the orginal articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301076d-d523-4c78-aa90-622b10ec67ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a142b96-91d5-434e-bcaf-c250e6c7f9be",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The purpose of this notebook was to provide some very intuitive ideas on how to use document vectors (particularly with TF-IDF weights) to implement important text mining methods, of course in a basic/simplified manner. Later we will cover other common methods that rely on vectorized text documents, including text clustering and text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a00326-0d89-4479-b0c5-7eb2e583884b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py310]",
   "language": "python",
   "name": "conda-env-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
