{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eeb7db4-03bb-4a57-ba94-da5e8263162a",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f415b-0a2d-4fc5-81a6-5ffd853a032a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis -- Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to effectively handle sequential data by retaining memory or context of previous inputs. Unlike feedforward neural networks that process data in fixed-size input vectors, RNNs have loops within their architecture, allowing them to maintain and utilize information about previous inputs while processing the current input.\n",
    "\n",
    "RNNs are composed of units (often called cells) that maintain a hidden state. This hidden state acts as a memory that retains information about previous inputs in the sequence. Each unit performs computations based on the current input and its previous hidden state, allowing them to capture temporal dependencies in sequential data.\n",
    "\n",
    "For text classification tasks, RNNs can be used in various ways:\n",
    "\n",
    "* **Word-level RNNs:** Each word in a text sequence is fed into the RNN step by step. The hidden state of the RNN unit at each step incorporates information about the previous words in the sequence. This way, the RNN learns to capture the context and dependencies between words in the text.\n",
    "\n",
    "* **Sequence-to-Sequence RNNs:** These models take an entire sequence as input and produce another sequence as output. In text classification, this could involve using an RNN to read an entire sentence or document and outputting a sentiment label or category.\n",
    "\n",
    "* **Sentiment Analysis:** In sentiment analysis, RNNs can be employed to classify the sentiment of text documents (positive, negative, neutral). The RNN processes the words or sequences of words in the document, learning patterns and relationships to determine the sentiment expressed.\n",
    "\n",
    "However, vanilla RNNs suffer from issues like vanishing or exploding gradients, which can hinder their ability to capture long-term dependencies in text. To address these limitations, variants of RNNs, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been developed. These architectures have gating mechanisms that control the flow of information, allowing them to better capture long-range dependencies in text sequences.\n",
    "\n",
    "In summary, RNNs, including LSTM and GRU variants, are powerful for text classification tasks because they can capture sequential dependencies, understand context, and make predictions based on the order and structure of text data. Their ability to retain memory and handle sequential information makes them well-suited for tasks where understanding the context of words or phrases is essential, such as sentiment analysis, named entity recognition, machine translation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821ac59-d8ef-4934-a49b-f3a08068dc08",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79138794-1ed9-4315-a122-77bca80859d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71fb95-a99c-42fb-a98e-ebb72a64c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Custom BatchSampler\n",
    "from src.sampler import EqualLengthsBatchSampler\n",
    "from src.utils import Dict2Class, plot_training_results\n",
    "from src.rnn import RnnType, RnnTextClassifier, DotAttentionClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbcc76-19d3-464e-97ad-a972e4ea10e6",
   "metadata": {},
   "source": [
    "### Checking/Setting the Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPU to significantly speed up the training process. If you have a support GPU, feel free to utilize it. However, for this notebook it's certainly not needed as our dataset is small and our network model is very simple. In fact, the training is fast on the CPU here since initializing memory on the GPU and moving the data to the GPU involves some overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f4cd4-66f7-48b8-b989-87399386266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU (in case you don't have a supported GPU)\n",
    "# With this small dataset and simple model you won't see a difference anyway\n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1021e-dbb7-4d8e-94db-5c37d0e0ea72",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "\n",
    "While RNNs allow for arbitrary lengths -- as long as sequence in the same batch is of the same length -- it is often practical to limited the maximum length of sequences. This is not only from a computing point of view but also it gets more and more difficult to propagate meaningful gradients back during Backpropgation Throught Time (BPTT).\n",
    "\n",
    "For the sentence dataset, this is hardly an isses, since individual sentences are usually not overly long. However, the moview reviews consiste of several sentences. Note that by limiting ourselves to the first `MAX_LENGTH` words we assume that the main sentiment is expressed at the beginning of the review. If we would assume that we should focus on the end of a review, we should consider the last `MAX_LENGTH` words. \n",
    "\n",
    "In the code cell below, we set `MAX_LENGTH` to 10, but feel free to play with this value. When loading the data from the files, we directly cut all sequences longer than `MAX_LENGTH` down to the specified values. This also means that we won't have to check the seqquence lengths anymore when training or evaluating a model (compared to CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38facef-5211-4345-8c02-3fd7aecc351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66b14c-3c6f-4144-8d27-c691b612af86",
   "metadata": {},
   "source": [
    "### Dataset A: Sentence Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f088595-f1c3-4ec5-8dd4-cc1148772578",
   "metadata": {},
   "source": [
    "#### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b5a2e-7a13-4613-a369-290eca1c599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torch.load(\"data/datasets/sentence-polarities/polarity-corpus-10000.vocab\")\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(\"Size of vocabulary:\\t{}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1614053-7b39-484c-90af-8775d6d870d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, targets = [], []\n",
    "\n",
    "with open(\"data/datasets/sentence-polarities/polarity-dataset-vectors-10000.txt\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        # The input sequences and class labels are separated by a tab\n",
    "        sequence, label = line.split(\"\\t\")\n",
    "        # Convert sequence string to a list of integers (reflecting the indicies in the vocabulary)\n",
    "        sequence = [ int(idx) for idx in sequence.split()]\n",
    "        # Convert each sequence into a tensor\n",
    "        sequence = torch.LongTensor(sequence[:MAX_LENGTH])\n",
    "        # Add sequence and label to the respective lists\n",
    "        sequences.append(sequence)\n",
    "        targets.append(int(label))\n",
    "        \n",
    "# As targets is just a list of class labels, we can directly convert it into a tensor\n",
    "targets = torch.LongTensor(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6e879-6234-4b45-914e-3a10e54b8c58",
   "metadata": {},
   "source": [
    "#### Create Training & Test Set\n",
    "\n",
    "To evaluate any classifier, we need to split our dataset into a training and a test set. With the method `train_test_split()` this is very easy to do; this method also shuffles the dataset by default, which is important for this example, since the dataset file is ordered with all positive sentences coming first. In the example below, we set the size of the test set to 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef731b-ee11-44c4-804c-2f223fc3f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "print(\"Number of training samples:\\t{}\".format(len(X_train)))\n",
    "print(\"Number of test samples:\\t\\t{}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5052b2c-2966-4257-b5c0-5d671b8f3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cf792-ab1e-4c1a-b1f9-26061e79e04a",
   "metadata": {},
   "source": [
    "### Dataset B: IMDb Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12bcb1-80bd-4318-bf4f-24fbcb2a3e8d",
   "metadata": {},
   "source": [
    "#### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670f072-11d4-4d66-90f0-4553fcc4f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torch.load(\"data/datasets/imdb-reviews/imdb-corpus-20000.vocab\")\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(\"Size of vocabulary:\\t{}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b759b3b-ce00-4750-9c04-fa9dd076d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train, samples_test = [], []\n",
    "\n",
    "with open(\"data/datasets/imdb-reviews/imdb-dataset-train-vectors-20000.txt\") as file:\n",
    "    for line in file:\n",
    "        name, label = line.split('\\t')\n",
    "        # Convert name to a sequence of integers\n",
    "        sequence = [ int(index) for index in name.split() ]\n",
    "        # Add (sequence,label) pair to list of samples\n",
    "        samples_train.append((sequence[:MAX_LENGTH], int(label.strip())))\n",
    "        \n",
    "#with open(\"data/imdb/aclimdb-sentiment-test-vectorized-20000.txt\") as file:\n",
    "with open(\"data/datasets/imdb-reviews/imdb-dataset-test-vectors-20000.txt\") as file:    \n",
    "    for line in file:\n",
    "        name, label = line.split('\\t')\n",
    "        # Convert name to a sequence of integers\n",
    "        sequence = [ int(index) for index in name.split() ]\n",
    "        # Add (sequence,label) pair to list of samples\n",
    "        samples_test.append((sequence[:MAX_LENGTH], int(label.strip())))\n",
    "        \n",
    "random.shuffle(samples_train)\n",
    "random.shuffle(samples_test)\n",
    "        \n",
    "print(\"Number of training samples: {}\".format(len(samples_train)))\n",
    "print(\"Number of test samples: {}\".format(len(samples_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3bca2-d6fb-4d1d-8294-e23cae26ef4c",
   "metadata": {},
   "source": [
    "#### Create Training & Test Set\n",
    "\n",
    "Since the dataset comes in 2 files reflecting the training and test data, we can directly convert the dataset into the respectice lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8096266-2736-4acc-a7c3-f62bec48f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [ torch.LongTensor(seq) for (seq, _) in samples_train ]\n",
    "X_test  = [ torch.LongTensor(seq) for (seq, _) in samples_test ]\n",
    "\n",
    "y_train = [ label for (_, label) in samples_train ]\n",
    "y_test  = [ label for (_, label) in samples_test ]\n",
    "\n",
    "# We can directly convert the vector of labels to a tensor\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test  = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccf913-3d54-43bc-a681-933436a7e7a6",
   "metadata": {},
   "source": [
    "### Create Dataset Class\n",
    "\n",
    "We first create a simple [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). This class only stores out `inputs` and `targets` and needs to implement the `__len__()` and `__getitem__()` methods. Since our class extends the abstract class [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), we can use an instance later to create a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Without going into too much detail, this approach does not only allow for cleaner code but also supports parallel processing on many CPUs, or on the GPU as well as to optimize data transfer between the CPU and GPU, which is critical when processing very large amounts of data. It is therefore the recommended best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c274ed-191f-4acf-aacc-7fa9dcea9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.targets is None:\n",
    "            return np.asarray(self.inputs[index])\n",
    "        else:\n",
    "            return np.asarray(self.inputs[index]), np.asarray(self.targets[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164f976-282d-4a14-881c-2df422be887e",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "\n",
    "The [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class takes a `DataSet` object as input to handle to split the dataset into batches. The class `EqualLengthsBatchSampler` analyzes the input sequences to organize all sequences into groups of sequences of the same length. Then, each batch is sampled for a single group, ensuring that all sequences in the batch have the same length. In the following, we use a batch size of 256, although you can easily go higher since we are dealing with only sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dffa6e-2aa4-4516-b037-70a583f4b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "dataset_train = BaseDataset(X_train, y_train)\n",
    "sampler_train = EqualLengthsBatchSampler(batch_size, X_train, y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_sampler=sampler_train, shuffle=False, drop_last=False)\n",
    "\n",
    "dataset_test = BaseDataset(X_test, y_test)\n",
    "sampler_test = EqualLengthsBatchSampler(batch_size, X_test, y_test)\n",
    "loader_test = DataLoader(dataset_test, batch_sampler=sampler_test, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada40ba5-55b7-43be-aca2-9364a20d99b5",
   "metadata": {},
   "source": [
    "## Train & Evaluate Model\n",
    "\n",
    "### Auxiliary Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36db0b-5763-4513-b423-4b3ab9ff2bd3",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "The code cell below implements the method `evaluate()` to, well, evaluate our model. Apart from the model itself, the method also receives the data loader as input parameter. This allows us later to use both `loader_train` and `loader_test` to evaluate the training and test loss using the same method.\n",
    "\n",
    "The method is very generic and is not specific to the dataset. It simply loops over all batches of the data loader, computes the log probabilities, uses these log probabilities to derive the predicted class labels, and compares the predictions with the ground truth to return the f1 score. This means, this method could be used \"as is\" or easily be adopted for all kinds of classifications tasks (incl. task with more than 2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d8973-b913-419b-8ed6-e73b04b2f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "            \n",
    "            # Move the batch to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Initialize the first hidden state h0 (and move to device)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            if type(hidden) is tuple:\n",
    "                hidden = (hidden[0].to(device), hidden[1].to(device))  # LSTM\n",
    "            else:\n",
    "                hidden = hidden.to(device)  # RNN, GRU\n",
    "                    \n",
    "            log_probs = model(X_batch, hidden)\n",
    "\n",
    "            y_batch_pred = torch.argmax(log_probs, dim=1)\n",
    "\n",
    "            y_true += list(y_batch.cpu())\n",
    "            y_pred += list(y_batch_pred.cpu())\n",
    "            \n",
    "            pbar.update(batch_size)\n",
    "\n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f7af5-8254-4a4c-a970-c0325b33803c",
   "metadata": {},
   "source": [
    "### Train Model (single epoch)\n",
    "\n",
    "Similar to the method `evaluate()` we also implement a method `train_epoch()` to wrap all the required steps training. This has the advantage that we can simply call `train_epochs()` multiple times to proceed with the training. Apart from the model, this method has the following input parameters:\n",
    "\n",
    "* `optimizer`: the optimizer specifier how the computed gradients are used to updates the weights; in the lecture, we only covered the basic Stochastic Gradient Descent, but there are much more efficient alternatives available\n",
    "\n",
    "* `criterion`: this is the loss function; \"criterion\" is just very common terminology in the PyTorch documentation and tutorials\n",
    "\n",
    "The hear of the method is the snippet described as PyTorch Magic. It consists of the following 3 lines of code\n",
    "\n",
    "* `optimizer.zero_grad()`: After each training step for a batch if have to set the gradients back to zero for the next batch\n",
    "\n",
    "* `loss.backward()`: Calculating all gradients using backpropagation\n",
    "\n",
    "* `optimizer.step()`: Update all weights using the gradients and the method of the specific optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396f9ae-5af6-467a-9223-ffa7b44563f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    \n",
    "    # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "\n",
    "            # Move the batch to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # Initialize the first hidden state h0 (and move to device)\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "\n",
    "            if type(hidden) is tuple:\n",
    "                hidden = (hidden[0].to(device), hidden[1].to(device))  # LSTM\n",
    "            else:\n",
    "                hidden = hidden.to(device)  # RNN, GRU            \n",
    "            \n",
    "            log_probs = model(X_batch, hidden)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(log_probs, y_batch)\n",
    "            \n",
    "            ### Pytorch magic! ###\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Keep track of overall epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            pbar.update(batch_size)\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1b176-2acb-4753-bf18-720d3caff0ae",
   "metadata": {},
   "source": [
    "#### Train Model (multiple epochs)\n",
    "\n",
    "The `train()` method combines the training and evaluation of a model epoch by epoch. The method keeps track of the loss, the training score, and the tests score for each epoch. This allows as later to plot the results; see below. Notice the calls of `model.train()` and `model.eval()` to set the models into the correcte \"mode\". This is needed sinze our model containsa Dropout layer. For more details, check out this [Stackoverflow post](https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86bae9-9ef8-4cf8-b3ad-18566f5348ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, loader_test, optimizer, criterion, num_epochs, verbose=False):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Total Training Time (total number of epochs: {})\".format(num_epochs))\n",
    "    #for epoch in tqdm(range(1, num_epochs+1)):\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = train_epoch(model, loader_train, optimizer, criterion)\n",
    "        model.eval()\n",
    "        f1_train = evaluate(model, loader_train)\n",
    "        f1_test = evaluate(model, loader_test)\n",
    "\n",
    "        results.append((epoch_loss, f1_train, f1_test))\n",
    "        \n",
    "        if verbose is True:\n",
    "            print(\"[Epoch {}] loss:\\t{:.3f}, f1 train: {:.3f}, f1 test: {:.3f} \".format(epoch, epoch_loss, f1_train, f1_test))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34458c13-748e-4e7a-85e8-af656c17d372",
   "metadata": {},
   "source": [
    "### Basic RNN Model\n",
    "\n",
    "The class `RnnTextClassifier` implements an RNN-based classifier in a flexible manner, using different parameters setting once cna set:\n",
    "\n",
    "* Which recurrent cell to use: nn.RNN, nn.GRU, or nn.LSTM\n",
    "\n",
    "* The number of stacked recurrent layers\n",
    "\n",
    "* Whether the recurrence is performed bi-directional or not\n",
    "\n",
    "* The number and size of the subsequence linear layers\n",
    "\n",
    "* ... and other various parameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f250c-a730-4d1c-9b0b-8b363148e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embed_size\": 300,\n",
    "    \"rnn_type\": RnnType.GRU,\n",
    "    \"rnn_num_layers\": 2,\n",
    "    \"rnn_bidirectional\": True,\n",
    "    \"rnn_hidden_size\": 512,\n",
    "    \"rnn_dropout\": 0.5,      # only relevant if rnn_num_layers > 1\n",
    "    \"dot_attention\": False,\n",
    "    \"linear_hidden_sizes\": [128, 64],\n",
    "    \"linear_dropout\": 0.5,\n",
    "    \"output_size\": 2\n",
    "}\n",
    "\n",
    "# Define model paramaters\n",
    "params = Dict2Class(params)\n",
    "# Create model   \n",
    "rnn = RnnTextClassifier(params).to(device)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n",
    "# Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08000a11-415b-41c9-8731-ccc26234cc76",
   "metadata": {},
   "source": [
    "#### Set Pretrained Word Embeddings (optional)\n",
    "\n",
    "If we want to use pre-trained word embeddings, e.g., Word2Vec, this is the moment to do. A source for pre-trained word embeddings is [this site](http://vectors.nlpl.eu/repository/). When downloading the a file containing pre-trained word embeddings, there are some things to consider:\n",
    "\n",
    "* Most obviously, the pre-trained embeddings should match the language (here: English).\n",
    "\n",
    "* The pretrained embeddings should match the preprocessing steps. For example, we lemmatized our dataset for this notebook (at least by default, maybe you have changed it). So we need embeddings trained over a lemmatized dataset as well.\n",
    "\n",
    "* The pretrained embeddings have to match the size of our embedding layer. So if we create a embedding layer of size 300, we have to use pretrained embeddings of the same size\n",
    "\n",
    "* The files with the pretrained embeddings are too large to ship with the notebooks, so you have to download them separately :)\n",
    "\n",
    "First, we need to load the pretrained embeddings from the file; here I used [this file](http://vectors.nlpl.eu/repository/20/5.zip) (lemmatized, size: 300):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35dc4b3-33c4-4abb-9d6c-113c55a935b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vectors = torchtext.vocab.Vectors(\"data/embeddings/model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0f113-256a-4605-be5c-753b8f36207e",
   "metadata": {},
   "source": [
    "Now we have over 270k pretrained word embeddings, but we only have 20k words in our vocabulary. So we need to create an embedding -- which is basically just a $20k \\times 300$ matrix containing the respective 20k pretrained word embeddings for our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df351e-534b-4d73-a426-0716297a7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = pretrained_vectors.get_vecs_by_tokens(vocabulary.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1db7f1-3036-4175-b3a8-33b8ea49e007",
   "metadata": {},
   "source": [
    "Now we can set the weights of the embedding layer of our model to the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0dbc8-fcdb-4c8b-afe4-08f61f9fc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f6b98-4ea4-4087-bc3f-a1f249fd7127",
   "metadata": {},
   "source": [
    "Lastly, we can decide if we want the pretrained embeddings to remain fixed or whether we want to update them during training. By setting `.requires_grad = False`, we tell the optimizer to \"freeze\" the layer **not** to update the embedding weights during training. You should observe that if we freeze the embedding layer, the training and test f1 score will remain quite similar; otherwise the training f1 score will go towards 1.0, indicating overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88816523-ed7d-434e-9ba0-9f32049b053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f92ca6-1065-42b6-a134-5fed4d7412c8",
   "metadata": {},
   "source": [
    "Since the embedding weights still reside on the CPU, we can move the model to the respective device so that the model on all data is indeed on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf909a0d-0d07-4c55-89b2-94620d712968",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba5cff-2a07-43f3-9ab2-aa8716e33066",
   "metadata": {},
   "source": [
    "#### Evaluate Untrained Model\n",
    "\n",
    "Let's first see how our model performs when untrain, i.e., with the initial random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b88e5-b624-4d81-880c-d0516db3f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rnn, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa6997-5956-4f70-9b19-efa9cc03842a",
   "metadata": {},
   "source": [
    "### Full Training (and evaluation after each epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740d46c-e28a-4952-ba03-ca57af7070ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "#train(basic_rnn_classifier, loader, num_epochs, verbose=True)\n",
    "results = train(rnn, loader_train, loader_test, optimizer, criterion, num_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67766399-fd0e-456d-a9f4-d8e72266cd87",
   "metadata": {},
   "source": [
    "In `src.utils` you can find the method `plot_training_results()` to plot the losses and accuracies (training + test) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f803c7d-c4b1-4567-b940-471d1e26ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0e42c-dbb3-4b21-9ac7-8fca96eb59b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b347f6-f78c-4a71-be91-bc8912a634c1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Recurrent Neural Networks (RNNs) have emerged as a formidable tool for text classification tasks like sentiment analysis due to their intrinsic ability to understand sequential data and capture dependencies among words or characters within text sequences. RNNs, unlike traditional feedforward networks, maintain a memory state that allows them to retain information from previous inputs while processing the current input. This unique architecture enables them to capture temporal dependencies and contextual information crucial for understanding the sentiment or meaning conveyed in text.\n",
    "\n",
    "In sentiment analysis, RNNs excel at grasping the sequential nature of language, discerning nuances in meaning, and identifying sentiment-bearing words or phrases within sentences or documents. By processing text sequentially, RNNs effectively consider the order of words and their relationships, thus grasping the context necessary for accurate classification.\n",
    "\n",
    "However, traditional RNNs are prone to issues like vanishing or exploding gradients, limiting their ability to capture long-range dependencies effectively. To mitigate these shortcomings, variants like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have been developed. These variants incorporate gating mechanisms that regulate the flow of information, allowing them to better retain relevant context over longer sequences.\n",
    "\n",
    "Overall, RNNs, including their specialized LSTM and GRU variants, stand out in text classification tasks like sentiment analysis by leveraging sequential information to comprehend context, relationships, and dependencies among words or characters within text data. Their strength lies in their capacity to handle sequences, making them a potent choice for tasks where understanding the sequential nature of language is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f832e-2035-413a-8957-1be3c568c78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
