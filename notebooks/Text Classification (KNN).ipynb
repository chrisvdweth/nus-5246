{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bbb8543-9b24-4bc3-b719-54bb2d02f05f",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19d0a9-13f7-408c-82f8-ae7dbef28125",
   "metadata": {},
   "source": [
    "# k-NN Classification\n",
    "\n",
    "The k-Nearest Neighbors (k-NN) algorithm is a simple and intuitive machine learning algorithm used for both classification and regression tasks. It operates based on the idea that similar data points are likely to belong to the same class or have similar characteristics. Here's an overview of the k-Nearest Neighbors algorithm and its application in text document classification:\n",
    "\n",
    "* **Algorithm Overview:** k-NN makes predictions by comparing a new, unseen data point (in this case, a text document) to the labeled data points (documents with known categories) in its training dataset. It calculates the similarity (often using distance metrics like Euclidean distance or cosine similarity) between the new document and all the documents in the training set.\n",
    "\n",
    "* **k Neighbors:** The \"k\" in k-NN represents the number of nearest neighbors (documents) to consider. To classify a new document, the algorithm identifies the k nearest neighbors in the training data based on their similarity to the new document.\n",
    "\n",
    "* **Voting or Weighted Averaging:** For classification tasks, once the k nearest neighbors are identified, k-NN uses either a voting mechanism or weighted averaging of their labels to determine the class of the new document. In voting, the class that occurs most frequently among the k neighbors is assigned to the new document. In weighted averaging, the neighbors' labels are weighted by their proximity or similarity to the new document.\n",
    "\n",
    "* **Text Document Classification:** For text document classification, the k-NN algorithm can be applied by representing documents as numerical vectors using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings. Each document becomes a point in a high-dimensional space, and the algorithm calculates distances or similarities between these points to find the nearest neighbors.\n",
    "\n",
    "* **Parameter Tuning:** The choice of k (the number of neighbors) is crucial and affects the model's performance. A smaller k may lead to more complex decision boundaries and could be more sensitive to outliers, while a larger k might oversmooth decision boundaries.\n",
    "\n",
    "* **Scalability and Efficiency:** While k-NN is straightforward and easy to understand, it might not be very efficient, especially with large datasets, as it requires calculating distances to every point in the training set for each prediction.\n",
    "\n",
    "In summary, k-Nearest Neighbors is a versatile algorithm suitable for text document classification by measuring similarities between documents. It can be effective for smaller datasets or when the decision boundaries between classes are relatively simple. However, its performance might degrade with high-dimensional data or when the dataset size becomes substantial due to computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767575d-c9d2-495f-88c8-5e9a0c5947d4",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Import Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0bab8a-49d2-46a4-b17f-79dcb5da5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ee3e4-6563-4cb6-98ae-8a507fe3de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95605cb-d78c-4176-aff3-89f787d4c559",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "For this notebook, we use a simple dataset for sentiment classification. This dataset consists of 10,662 sentences, where 50% of the sentences are labeled 1 (positive), and 50% of the sentences are labeled -1 (negative).\n",
    "\n",
    "### Loading Sentence/Label Pairs from File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4014a-4e71-4705-b448-35ed3ffb22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = [], []\n",
    "\n",
    "with open(\"data/datasets/sentence-polarities/sentence-polarities.csv\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        sentence, label = line.split(\"\\t\")\n",
    "        sentences.append(sentence)\n",
    "        labels.append(int(label))  \n",
    "        \n",
    "print(\"Total number of sentences: {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728c8fc-19f5-46b8-b4ae-c5020a54c0d4",
   "metadata": {},
   "source": [
    "### Create Training & Test Set\n",
    "\n",
    "To evaluate any classifier, we need to split our dataset into a training and a test set. With the method `train_test_split()` this is very easy to do; this method also shuffles the dataset by default, which is important for this example, since the dataset file is ordered with all positive sentences coming first. In the example below, we set the size of the test set to 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8e763-c7f7-44db-ab32-e799eab2ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences and labels into training and test set with a test set size of 20%\n",
    "sentences_train, sentences_test, labels_train, labels_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# We can directly convert the numerical class labels from lists to numpy arrays\n",
    "y_train = np.asarray(labels_train)\n",
    "y_test = np.asarray(labels_test)\n",
    "\n",
    "print(\"Size of training set: {}\".format(len(sentences_train)))\n",
    "print(\"Size of test set: {}\".format(len(sentences_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec73c2-0567-46da-9f04-9393e8a933ce",
   "metadata": {},
   "source": [
    "## Training & Testing a k-NN Classifier\n",
    "\n",
    "Let's first have a look at how to train a k-NN classifier with the minimum number of steps. This includes that we simply pick some values for all hyperparameters which in the case of k-NN is simply the number of nearest neighbors $k$. Of course, we should also not forget that there are different hyperparameters when it comes to converting our corpus into the Document-Term Matrix. But let's just assume the following setting for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5cb848-5324-4f6c-bb57-8fa12f965b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document-Term Matrix for differen n-gram sizes\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "\n",
    "X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "X_test = tfidf_vectorizer.transform(sentences_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef9346-5773-4d17-9bbc-474cd60df1f4",
   "metadata": {},
   "source": [
    "Using the training data, we can train a k-NN classifier with a single line of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd90a07-2ef2-4085-a070-4a62f2c57636",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=20).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db8bf4-dbdb-41cd-9851-8c073a845e1f",
   "metadata": {},
   "source": [
    "Once trained, we can predict the class labels for the document vectors in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c60195-54ee-4982-82dc-ffe902a9b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f6439-b444-4477-921b-47e8aaf6acef",
   "metadata": {},
   "source": [
    "`y_pred` now contains the 2,133 predicted labels that we can compare with the ground truth labels from the test set. scikit-learn provides methods to easily calculate all the important metrics we covered in the lecture. Since we only have to class labels (i.e., binary classification), we do not have to set the `average` parameter to indicate micro or macro averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03669c7f-8804-439e-bdc2-bbb92029db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precison: {:.3f}\".format(precision))\n",
    "print(\"Recall:   {:.3f}\".format(recall))\n",
    "print(\"F1 score: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13cf18-70b3-4df1-8172-c27c79c740a8",
   "metadata": {},
   "source": [
    "scikit-learn also provides a method `classification_report()` for a more detailed description of the results, showing a breakdown of the precision, recall, and f1 scores broken down for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2eae60-292b-43ed-879f-f10e6f063126",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb070f85-59c5-4ef6-b6aa-e771a0ca6445",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "In the example above, we simply set $k=20$ in the hopes that this will yield good results. In practice, of course, we are interested in finding the best values for $k$ systematically and not just guessing it. This is where hyperparameter tuning and cross validation comes into play.\n",
    "\n",
    "### Selecting the Right K (only)\n",
    "\n",
    "Let's first consider $k$ as the only hyperparameter. To find its best value, we have to try different choices, perform cross validation for the choice of $k$, and keep track of the results. The loop in the code cell below accomplishes this. Note the method `cross_val_score()` from scikit-learn that automatically performs k-fold cross validation with a single line of code. In the example below, we perform 10-fold cross validations (`cv=10`). Since we get 10 f1 scores, one for each fold, we compute the final f1 score as the average over the scores of all folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659f946-b417-418e-9d93-d6fb797c2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_neighbors = 100\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "# Loop over all odd values between 1 and max_num_neighbors+1\n",
    "for k in tqdm(range(1, max_num_neighbors+1, 2)):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring=\"f1\")\n",
    "    mean_score = np.mean(scores)\n",
    "    \n",
    "    f1_scores.append((k, mean_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb1346-071e-4b5b-9a5a-1c1bebcb4544",
   "metadata": {},
   "source": [
    "In the code cell above, we simply store all combinations of $k$ values and their corresponding f1 scores. This allows us to easily plot the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558a709-a4a6-4e84-9826-24916b8217ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([s[0] for s in f1_scores], [s[1] for s in f1_scores], lw=3)\n",
    "font_axes = {'family':'serif','color':'black','size':16}\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"Number of Neighbors k\", fontdict=font_axes)\n",
    "plt.ylabel(\"F1 Score\", fontdict=font_axes)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f5556-126e-4de6-b405-3c9d09d7c42d",
   "metadata": {},
   "source": [
    "From the plot we can see that the f1 scores initially improve quite quickly, but around $k=40$ seem starting to converge, and later even slightly drop again.\n",
    "\n",
    "We could now pick our choice of $k$ by eye-balling the plot above, or we can find the best choice of $k$ also programmatically. With the data we already have, we can first find the index position in the result list that corresponds to the highest f1 scores. We can then look up the respective $k$ values. The code cell below accomplishes this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff8e0f-a8d9-406c-bacb-e1f0660941a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = np.asarray([ t[0] for t in f1_scores ])\n",
    "f1_values = np.asarray([ t[1] for t in f1_scores ])\n",
    "\n",
    "# Find the indices referring to the largest f1 scores\n",
    "indices_sorted = np.argsort(f1_values)[::-1]\n",
    "\n",
    "# Show k values sorted w.r.t. the f1 scores in descending order\n",
    "print(k_values[indices_sorted])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7bcbf-0d9a-46ab-b7f6-402001259683",
   "metadata": {},
   "source": [
    "This results tells us that for $k=67$ we will get the largest f1 score. However, we can also see from the plot that all top results are rather similar, and our dataset is not very large to make a really good judgment. Nevertheless, let's work with $k=67$. Having identified this as our best choice for $k$, we can now train a k-NN classifier with this value over the whole training data, and finally calculate the f1 score using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0cfaa-9ebc-42eb-bc31-2da4f30276b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=67).fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precison: {:.3f}\".format(precision))\n",
    "print(\"Recall:   {:.3f}\".format(recall))\n",
    "print(\"F1 score: {:.3f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951bdf3-2cc6-4812-96d2-bfcf3eb5a143",
   "metadata": {},
   "source": [
    "This looks obvious better then our initial choice of $k=20$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393bb0a-759c-4616-9bcf-07695d21f798",
   "metadata": {},
   "source": [
    "### Selecting the Right K and Maximum N-Gram Size\n",
    "\n",
    "So far, we used the same document vectors for finding the best value of $k$. However, we know that we already have options when it comes to feature extractions (you can check out all the input parameters of [`TfidfVectorizer`](https://www.google.com/search?channel=fs&client=ubuntu&q=sklearn+tfidfvectorizer)). In principle, we could consider all possible parameters. However, the search space would quickly explode. So let's just consider the maximum size of n-grams. To further limit the number of runs, we further only consider a subset of possible values for $k$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037939d-a926-43de-8e01-9979dce54d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_sizes = [7, 17, 27, 37, 47, 57, 67, 77, 87, 97 ]\n",
    "#k_sizes = [7, 17, 27]\n",
    "\n",
    "max_ngram_size = 5\n",
    "#max_ngram_size = 3\n",
    "num_k = len(k_sizes)\n",
    "\n",
    "# Number runs = number traing/test a k-NN classifier\n",
    "num_runs = max_ngram_size * num_k\n",
    "\n",
    "# numpy array to keep track of all results\n",
    "knn_results = np.zeros((max_ngram_size, num_k))\n",
    "\n",
    "with tqdm(total=num_runs) as pbar:\n",
    "    for i, ngram in enumerate(range(1, max_ngram_size+1)):\n",
    "        # Create Document-Term Matrix for different n-gram sizes\n",
    "        tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, ngram), max_features=20000)\n",
    "        X_train = tfidf_vectorizer.fit_transform(sentences_train)\n",
    "        X_test = tfidf_vectorizer.transform(sentences_test)\n",
    "        # Train & test model using cross validation\n",
    "        for j, k in enumerate(k_sizes):\n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            scores = cross_val_score(knn, X_train, y_train, cv=10, scoring=\"f1\")\n",
    "            mean_score = np.mean(scores)\n",
    "            knn_results[i,j] = mean_score\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3272a6-2ba1-48c6-b9dd-8e67439194a5",
   "metadata": {},
   "source": [
    "Now that we performed hyperparameter tuning for 2 parameters at the same time, a simple line plot is no longer suitable. The code cell below plots a heatmap, indicating the combination of maximum n-gram size and $k$ gave us the highest f1 score(s). The Python package `seaborn` makes plotting heatmaps quite easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0074f52-575e-4b32-b34f-d8f894692be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the heatmap function from the seaborn package\n",
    "plt.figure()\n",
    "\n",
    "sns.heatmap(knn_results, annot=True, cmap=\"crest\", xticklabels=k_sizes, yticklabels=list(range(1,max_ngram_size+1)))\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('Maximum N-Gram Size', fontsize=16)\n",
    "plt.xlabel('Number of Neighbors (k)', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ace1a-ac44-4769-a922-0ccbdecfd2e4",
   "metadata": {},
   "source": [
    "When looking at this plot, it seems that using just unigrams and our initially best value of $k=67$ seems to do just fine. Of course, in principle, we would need to check other parameters such as `max_features` or `min_df` as well. Depending on the runtime of a single training and cross validation step, we might not try all possible combinations. Instead, we would tune 1-2 parameters at the same time, keeping all other parameters fixed. Once having identified meaningful values for those parameters, we keep this fixed and tune another choice of 1-2 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d086fbb-fb04-4ea0-92d2-263ad01cdbbe",
   "metadata": {},
   "source": [
    "## Pipelines & Grid Search\n",
    "\n",
    "Hyperparameter tuning is a quite important step, but the previous example has shown that it can be quite tedious. However, note that we basically tried all possible combinations for certain sets of parameter values. And since we were tuning 2 parameters, we required 2 nested loops. Thus, if we would tune $N$ parameters at the same time, we would need to have $N$ nested loops. Luckily, scikit-learn makes this much easier using [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "Since the parameters we would like to tune refer to 2 different components -- the vectorizer and the classifier -- we also need a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to combine both components into a single model. Let's do this first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c7b3e-1420-47c1-9754-2758b5d5684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a091c-297e-4b3c-aa6e-74cdb624eeae",
   "metadata": {},
   "source": [
    "Now we can define the search space, by providing the set of values for the hyperparameters we want to consider. See how the identifier of the parameters are a combination of the name in the pipeline (here: `tfidf` and `knn`) and the name of the parameter in the respective class. For example, `tfidf__max_df` refers to the `max_df` parameter of the `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e536653-6517-441d-be24-3c04b0324547",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.75, 1.0),\n",
    "    'tfidf__max_features': (5000, 10000),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),\n",
    "    'knn__n_neighbors': [57, 67, 77]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc595c-5ddb-4f59-8d8a-9814d9908792",
   "metadata": {},
   "source": [
    "Now we can use `GridSearchCV` to check all possible combinations of parameter values. Of course, we kept the number of possible values rather small to avoid overly run times here. Note that for any parameter not listed above (e.g., `min_df` of the vectorizer) the default value is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a7fcb4-df11-49e9-970e-27a027235968",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=2, cv=5)\n",
    "\n",
    "grid_search = grid_search.fit(sentences_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698655b5-2a5c-4bea-9344-6e3cb4f2e97a",
   "metadata": {},
   "source": [
    "Once the `GridSearchCV` has checked all possible parameter combinations, we can read out the best combination as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7777bc-075f-4431-930b-031299a7f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629210f3-e926-4feb-b5ff-82ba126d5b71",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b665c9-7529-492d-babd-45d3b7c30ebe",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The k-Nearest Neighbors (k-NN) algorithm is a straightforward yet powerful method used in text classification tasks. It operates on the principle of similarity, making predictions for new text documents based on their resemblance to known labeled documents in a training set.\n",
    "\n",
    "In text classification, k-NN treats each document as a data point in a high-dimensional space, where the dimensions represent features such as word frequencies or TF-IDF values. When a new document needs classification, k-NN calculates its similarity to all documents in the training set using distance metrics like Euclidean distance or cosine similarity. The algorithm identifies the k most similar documents (neighbors) to the new document. These neighbors contribute to the classification decision by either voting (assigning the class label most common among the neighbors) or weighted averaging of their labels based on their proximity to the new document.\n",
    "\n",
    "One of the strengths of k-NN in text classification lies in its simplicity and ease of implementation. It doesn't require assumptions about the underlying data distribution and can handle multi-class classification effectively. However, its performance can be affected by the choice of k—the number of neighbors considered—where a smaller k might lead to more complex decision boundaries, while a larger k might oversimplify them. Additionally, k-NN can face challenges with scalability and computational efficiency, especially with large datasets, as it needs to compute distances to all training instances for each prediction.\n",
    "\n",
    "Overall, k-Nearest Neighbors is a valuable algorithm for text classification, providing a intuitive way to categorize new documents based on their similarity to existing labeled documents in the training set. Its applicability in smaller datasets and its simplicity make it a useful baseline method in text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e3ded-63a1-4009-b8c9-97f3698926cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
