{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7645cd5d-1bfa-4c75-8e56-f59036fea061",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5a8fb-c8fa-471b-8831-17b962bf5a6b",
   "metadata": {},
   "source": [
    "# RNN-Based Named Entity Recognition (NER)\n",
    "\n",
    "Training a Recurrent Neural Network (RNN) for Named Entity Recognition (NER) involves the following steps:\n",
    "\n",
    "* **Data Preparation:** Prepare the training data for the RNN by dividing it into input and output sequences. Each input sequence should correspond to a sentence in the text, and each output sequence should correspond to the corresponding labels for the named entities in the sentence. (This step we did in the previous notebook.)\n",
    "\n",
    "* **Word Embedding:** Convert the input sequences into word embeddings, which are vector representations of words that capture semantic and syntactic information. Pre-trained word embeddings such as Word2Vec or GloVe can be used, or the embeddings can be trained from scratch using the training data.\n",
    "\n",
    "* **Model Architecture:** Define the architecture of the RNN, including the number of layers, the type of RNN (such as LSTM or GRU), and the number of neurons in each layer. The output layer should have one neuron for each possible named entity label.\n",
    "\n",
    "* **Training:** Train the RNN using the prepared data and the defined architecture. This involves optimizing the model's parameters (weights and biases) to minimize the loss function, which measures the difference between the predicted labels and the true labels.\n",
    "\n",
    "* **Evaluation:** Evaluate the performance of the trained RNN on a validation set, using metrics such as precision, recall, and F1 score. Adjust the model architecture and training parameters as necessary to improve performance.\n",
    "\n",
    "* **Prediction:** Use the trained RNN to predict the named entities in new text data, by feeding the input sequences through the model and extracting the output labels.\n",
    "\n",
    "It is important to note that training an RNN for NER requires a large amount of labeled data, and can be computationally intensive. It is also important to carefully tune the hyperparameters of the model, such as the learning rate and batch size, to ensure good performance.\n",
    "\n",
    "In this notebook, we will go through some of those basic steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe53f20-3f28-4170-b3c5-2616698e5812",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c26aa3-777b-402f-a144-f14839b19376",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31408a9a-949b-4a56-93ef-0050518b3161",
   "metadata": {},
   "source": [
    "### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86f9ab9-9dba-441d-bc18-2af87a265ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad249c1-31ce-4658-be47-7271c7677c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import Dict2Class\n",
    "from src.sampler import BaseDataset, EqualLengthsBatchSampler\n",
    "from src.rnn import VanillaRnnNER, PosRnnNER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c192f8-84ea-4d6b-85ad-4553bbe50040",
   "metadata": {},
   "source": [
    "### Checking/Setting the Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPU to significantly speed up the training process. If you have a support GPU, feel free to utilize it. However, for this notebook it's certainly not needed as our dataset is small and our network model is very simple. In fact, the training is fast on the CPU here since initializing memory on the GPU and moving the data to the GPU involves some overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0f12b-3513-45b5-87a1-cd6413daccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f93af-f6b7-4f22-a362-ee5b59ff6e82",
   "metadata": {},
   "source": [
    "## Load & Prepare Dataset\n",
    "\n",
    "In the previous notebook, we prepared our dataset for the training. This mainly meant that we created the vocabularies and vectorized the words as well as the POS tags. So here, we now only have to load the vocabularies and the vectorized sequences and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194dbe9-0558-4d28-99aa-b0ee96bd6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_words = torch.load(\"data/datasets/gmb-ner/gmb-ner-token.vocab\")\n",
    "vocab_pos = torch.load(\"data/datasets/gmb-ner/gmb-ner-pos.vocab\")\n",
    "vocab_label = torch.load(\"data/datasets/gmb-ner/gmb-ner-label.vocab\")\n",
    "\n",
    "vocab_size_words = len(vocab_words)\n",
    "vocab_size_pos = len(vocab_pos)\n",
    "vocab_size_label = len(vocab_label)\n",
    "\n",
    "print(\"Size of word vocabulary:\\t{}\".format(vocab_size_words))\n",
    "print(\"Size of POS vocabulary:\\t{}\".format(vocab_size_pos))\n",
    "print(\"Size of TAG vocabulary:\\t{}\".format(vocab_size_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b1db5-c761-43a7-a3bf-b982ae628c33",
   "metadata": {},
   "source": [
    "For the training, we want to consider 2 models, where the first model is using the words as input and the second model is using the words and the POS tags as input. To this end, we load the vectorized sequences into 2 lists. Recall that simply concatenated the sequence of token indices and the sequences of POS tag indices for each sentence. This, the data for the first model, we \"cut the sequences into half\" to only deal with the token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d8594-0371-481c-9810-fea6cae723d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_vanilla, samples_pos = [], []\n",
    "\n",
    "num_sentences = sum(1 for i in open(\"data/datasets/gmb-ner/gmb-ner-data-vectorized.txt\", \"rb\"))\n",
    "\n",
    "print(num_sentences)\n",
    "\n",
    "with open(\"data/datasets/gmb-ner/gmb-ner-data-vectorized.txt\") as file:\n",
    "    with tqdm(total=num_sentences) as t:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            inputs, targets = line.split(\",\")\n",
    "            # Convert name to a sequence of integers\n",
    "            input_seq_pos = [ int(index) for index in inputs.split() ]\n",
    "            input_seq_vanilla = input_seq_pos[:len(input_seq_pos)//2]\n",
    "            target_seq = [ int(index) for index in targets.split() ]\n",
    "            # Add (sequence,label) pair to list of samples\n",
    "            samples_vanilla.append((input_seq_vanilla, target_seq))\n",
    "            samples_pos.append((input_seq_pos, target_seq))\n",
    "            t.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70518e7-2cad-4499-adf4-4e49355d9051",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afac87-4ddd-464b-8446-b41939fe6948",
   "metadata": {},
   "source": [
    "## Vanilla RNN for NER\n",
    "\n",
    "### Prepare Training & Test Data\n",
    "\n",
    "For the first model, we only consider the words/tokens as input. The image below, taken from the lecture slides, shows the overall architecture.\n",
    "\n",
    "<img width=\"80%\" src=\"data/images/ner-rnn-basic-architecture.png\">\n",
    "\n",
    "Let's first create the basic dataset from the data we have just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a6c86-73cd-46bb-a877-09facea77b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ torch.LongTensor(inputs) for (inputs, _) in samples_vanilla ]\n",
    "Y = [ torch.LongTensor(targets) for (_, targets) in samples_vanilla ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172d9acf-cfbc-4923-883e-d5be2b85c2e5",
   "metadata": {},
   "source": [
    "In this notebook, we won't perform a proper evaluation since evaluating NER models can be quite tricky (cf. lecture slides). We therefore can consider most samples for the training and just preserve some test samples for predicting the NER labels for some sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268364e5-1053-406c-99ff-03330b517c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01, shuffle=True, random_state=0)\n",
    "\n",
    "print(\"Number of training samples:\\t{}\".format(len(X_train)))\n",
    "print(\"Number of test samples:\\t\\t{}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66457b9-2ca7-47eb-8586-9b0bc1941441",
   "metadata": {},
   "source": [
    "Lastly, we create the data loaders for convenient handling of the data when training the model. Again, we discussed these steps and utility classes in full detail in previous notebooks, so we skip those details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c804f7d-df68-4b88-8446-6eb27830a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 1\n",
    "\n",
    "dataset_train = BaseDataset(X_train, Y_train)\n",
    "sampler_train = EqualLengthsBatchSampler(batch_size_train, X_train, Y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_sampler=sampler_train, shuffle=False, drop_last=False)\n",
    "\n",
    "dataset_test = BaseDataset(X_test, Y_test)\n",
    "sampler_test = EqualLengthsBatchSampler(batch_size_test, X_test, Y_test)\n",
    "loader_test = DataLoader(dataset_test, batch_sampler=sampler_test, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e5ea4-cf6c-428b-a5fb-521b03d91747",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "Both models considered in this notebook can be found in the file `src/rnn.py`. Both models are implemented in a way to make them easily configurable. This means that there are various parameters you can set to specify the exact network architectures. Since this first model only considers the word/token features, this model has slightly less parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d28fd-cd83-412f-8906-5f7768f155d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size_words = 100\n",
    "embed_size_pos = 50\n",
    "\n",
    "bilstm_hidden_size = 256\n",
    "bilstm_num_layers = 1\n",
    "bilstm_dropout = 0.2\n",
    "\n",
    "params = {\n",
    "    \"device\": device,\n",
    "    \"vocab_size_words\": vocab_size_words,\n",
    "    \"vocab_size_label\": vocab_size_label,\n",
    "    \"embed_size\": embed_size_words,\n",
    "    \"bilstm_hidden_size\": bilstm_hidden_size,\n",
    "    \"bilstm_num_layers\": bilstm_num_layers,\n",
    "    \"bilstm_dropout\": bilstm_dropout,\n",
    "    \"linear_hidden_sizes\": [256, 128],\n",
    "    \"linear_dropout\": 0.2\n",
    "}\n",
    "\n",
    "params = Dict2Class(params)\n",
    "\n",
    "model = VanillaRnnNER(params).to(device)\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087d1d5",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "In the file `src/utils.py` we provide the method `train()` to train the model. This method contains all required training steps that we have seen multiple times in other lecture notebooks before. Feel free to check out the method to remind yourself of what it does. This method returns a list containing the losses for each epoch. We will later use this for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a85831-73bb-4c31-86ec-f18c27f8d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, loader, optimizer, criterion, num_epochs, device, verbose=False):\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    # Set model to \"train\" mode\n",
    "    model.train()\n",
    "    \n",
    "    print(\"Total Training Time (total number of epochs: {})\".format(num_epochs))\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        with tqdm(total=len(loader)) as pbar:\n",
    "        \n",
    "            for inputs, targets in loader:\n",
    "                batch_size, seq_len = inputs.shape[0], inputs.shape[1]\n",
    "\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)                \n",
    "\n",
    "                loss = criterion(outputs.permute(0,2,1), targets)\n",
    "\n",
    "                ### Pytorch magic! ###\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Keep track of overall epoch loss\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "                \n",
    "        if verbose is True:\n",
    "            print(\"Loss:\\t{:.3f} (epoch {})\".format(epoch_loss, epoch))\n",
    "            \n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5b009-ed5e-49b1-a356-1820987cc86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "losses_vanilla = train(model, loader_train, optimizer, criterion, num_epochs, device, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99545e9-6a7e-46d4-82e9-b292cc497d76",
   "metadata": {},
   "source": [
    "### Test Model\n",
    "\n",
    "Although we don't perform a proper evaluation, we can simply predict the NER labels for sentences from the test set. For this, we first need to define a method `eval_sample()` that predicts the NER labels for a given sentence and a given model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07514e0-c166-49cb-8c6a-b3b02f2b7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sample(X, Y, model):\n",
    "    model.eval()\n",
    "    \n",
    "    # We assume a batch size of 1!!!\n",
    "    outputs = model(X.to(device)).squeeze(0)\n",
    "    \n",
    "    # Print: predicted label / true label / word\n",
    "    for idx in range(outputs.shape[0]):\n",
    "        _, topi = outputs[idx].topk(1)\n",
    "        label_pred = vocab_label.lookup_token(topi)\n",
    "        label_true = vocab_label.lookup_token(Y[0][idx])\n",
    "        word = vocab_words.lookup_token(X[0][idx])\n",
    "        print(\"{}\\t{}\\t{}\".format(label_pred, label_true, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e52c2-f98f-4b84-9194-2e6f49d853c6",
   "metadata": {},
   "source": [
    "The code cell below picks a random sentence from the test set and predicts the NER labels. Just run the code cell multiple times for different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d71636-1885-4a9a-8e30-399ebf207e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in loader_test:\n",
    "    eval_sample(X, Y, model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d8a3a-e31a-46c0-98cf-246d9b18e152",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f51f4f",
   "metadata": {},
   "source": [
    "## RNN for NER with POS Tags\n",
    "\n",
    "We now perform the same steps of creating a training and test dataset suitable for the modified RNN architecture to also consider the POS tags of words. The figure below shows the modified basic architecture that now takes both individual words as well as their POS tags as input features.\n",
    "\n",
    "<img width=\"80%\" src=\"data/images/ner-rnn-pos-architecture.png\">\n",
    "\n",
    "Let's first create the dataset from the data that also includes the POS tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ torch.LongTensor(inputs) for (inputs, _) in samples_pos ]\n",
    "Y = [ torch.LongTensor(targets) for (_, targets) in samples_pos ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ca498",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.01, shuffle=True, random_state=0)\n",
    "\n",
    "print(\"Number of training samples:\\t{}\".format(len(X_train)))\n",
    "print(\"Number of test samples:\\t\\t{}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe50ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 64\n",
    "batch_size_test = 1\n",
    "\n",
    "dataset_train = BaseDataset(X_train, Y_train)\n",
    "sampler_train = EqualLengthsBatchSampler(batch_size_train, X_train, Y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_sampler=sampler_train, shuffle=False, drop_last=False)\n",
    "\n",
    "dataset_test = BaseDataset(X_test, Y_test)\n",
    "sampler_test = EqualLengthsBatchSampler(batch_size_test, X_test, Y_test)\n",
    "loader_test = DataLoader(dataset_test, batch_sampler=sampler_test, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855d5ca",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "As this model is a bit more complex, it also offers more input parameters for its configuration. Again, you can check out the full implementation of the model in the file `src/rnn.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc03515",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size_words = 100\n",
    "embed_size_char = 50\n",
    "embed_size_pos = 50\n",
    "\n",
    "bilstm_hidden_size = 256\n",
    "bilstm_num_layers = 1\n",
    "bilstm_dropout = 0.2\n",
    "\n",
    "params = {\n",
    "    \"device\": device,\n",
    "    \"vocab_size_words\": vocab_size_words,\n",
    "    \"vocab_size_pos\": vocab_size_pos,\n",
    "    \"vocab_size_tag\": vocab_size_label,\n",
    "    \"embed_size_words\": embed_size_words,\n",
    "    \"embed_size_pos\": embed_size_pos,\n",
    "    \"bilstm_hidden_size\": bilstm_hidden_size,\n",
    "    \"bilstm_num_layers\": bilstm_num_layers,\n",
    "    \"bilstm_dropout\": bilstm_dropout,\n",
    "    \"linear_hidden_sizes\": [256],\n",
    "    \"linear_dropout\": 0.2\n",
    "}\n",
    "\n",
    "params = Dict2Class(params)\n",
    "\n",
    "model = PosRnnNER(params).to(device)\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c537ea4",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Training this model simply involves calling the `train()` method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "losses_pos = train(model, loader_train, optimizer, criterion, num_epochs, device, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a1161",
   "metadata": {},
   "source": [
    "If you want, you can again run the code cell below multiple times to see how the models perform on sentences from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, Y in loader_test:\n",
    "    eval_sample(X, Y, model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8740e0b",
   "metadata": {},
   "source": [
    "## Compare Models\n",
    "\n",
    "A proper comparison of both models would require a proper evaluation of the models which is beyond the scope here. However, we can compare the losses for each epoch of both models to get at least some crude insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1, len(losses_vanilla)+1))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, losses_vanilla, lw=3)\n",
    "plt.plot(x, losses_pos, lw=3)\n",
    "\n",
    "font_axes = {'family':'serif','color':'black','size':16}\n",
    "\n",
    "plt.gca().set_xticks(x)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontdict=font_axes)\n",
    "plt.ylabel(\"Loss\", fontdict=font_axes)\n",
    "plt.legend(['Vanilla RNN', 'POS RNN'], loc='upper right', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a41aafe-e400-4870-a972-4cdaaf2e3b07",
   "metadata": {},
   "source": [
    "As the plot shows, the losses for the POS RNN decrease/converge faster than the losses for the Vanilla RNN. Again, while this is not a proper evaluation, it does indicate that considering the POS tags does result in a more effective training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1d2c6-b395-4726-9613-e4d23aba3f0e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b66f3-430b-4034-8001-24c9d8335a9a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Recurrent neural networks (RNNs) are a type of neural network commonly used for natural language processing (NLP) tasks, including named entity recognition (NER). RNNs are well-suited for NER because they can capture the contextual dependencies between words in a sentence, which is important for accurately identifying named entities.\n",
    "\n",
    "To train an RNN for NER, the data is first prepared by dividing it into input and output sequences. The input sequences correspond to sentences in the text, and the output sequences correspond to the labels for the named entities in the sentence. The input sequences are then converted into word embeddings, which are vector representations of words that capture semantic and syntactic information.\n",
    "\n",
    "The RNN architecture is then defined, which typically includes multiple layers of LSTM or GRU units. The output layer has one neuron for each possible named entity label. The model is then trained using the prepared data, optimizing the parameters to minimize the loss function. Once trained, the performance of the RNN is evaluated on a validation set using metrics such as precision, recall, and F1 score. The model is then used to predict the named entities in new text data.\n",
    "\n",
    "One advantage of using RNNs for NER is that they can handle variable-length input sequences, which is important for processing text data. Additionally, RNNs can be used with pre-trained word embeddings, which can improve performance and reduce the amount of data required for training. Overall, RNNs are a powerful tool for NER, and have been used successfully in a wide range of NLP applications. However, training an RNN for NER can be computationally intensive, and requires a large amount of labeled data and careful tuning of hyperparameters to achieve good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380c46a-6003-4a2c-a199-7473aeb394bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
