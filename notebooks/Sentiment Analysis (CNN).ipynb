{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a892fb-149e-475a-a2e6-87aaaca69a21",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f415b-0a2d-4fc5-81a6-5ffd853a032a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis -- Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a type of deep learning model primarily used for analyzing visual imagery. However, they've also been adapted for text classification tasks.\n",
    "\n",
    "CNNs are composed of layers that detect patterns within data by using convolutional operations. These layers consist of filters or kernels that slide across input data, capturing spatial hierarchies of features. In image processing, these features might represent edges, textures, or more complex structures.\n",
    "\n",
    "When applied to text, CNNs can be used to detect patterns in sequences of words. Text data is converted into numerical representations like word embeddings. These embeddings capture the relationships between words, enabling the CNN to analyze sequences of these representations. Here's a basic overview of how CNNs can be used for text classification:\n",
    "\n",
    "* **Input Encoding:** Text documents are tokenized into words or characters and converted into numerical representations (word embeddings, character embeddings, etc.). This we already handled in the Data Preparation notebook.\n",
    "\n",
    "* **Convolutional Layers:** Similar to image processing, convolutional layers in a text-based CNN apply filters over sequences of word embeddings to detect patterns or features. These filters slide across the sequence, capturing local patterns.\n",
    "\n",
    "* **Pooling Layers:** After convolutions, pooling layers (e.g., max pooling) downsample the output, extracting the most important information and reducing dimensionality.\n",
    "\n",
    "* **Fully Connected Layers:** These layers take the processed features and perform classification, often using techniques like softmax for multi-class classification, assigning probabilities to different classes.\n",
    "\n",
    "* **Training:** The network is trained using labeled data (text documents with their corresponding categories or labels) to optimize its parameters (weights and biases) via backpropagation and gradient descent.\n",
    "\n",
    "CNNs can capture local and hierarchical patterns in text data, learning representations that can discern different classes or categories in documents. They're particularly effective for tasks where local word order matters, such as sentiment analysis, topic classification, or spam detection. While recurrent neural networks (RNNs) and transformers are also popular for text analysis, CNNs offer advantages in computational efficiency and parallel processing, making them suitable for certain text classification tasks, especially when considering local contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821ac59-d8ef-4934-a49b-f3a08068dc08",
   "metadata": {},
   "source": [
    "## Setting up the Notebook\n",
    "\n",
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79138794-1ed9-4315-a122-77bca80859d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71fb95-a99c-42fb-a98e-ebb72a64c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom BatchSampler\n",
    "from src.sampler import EqualLengthsBatchSampler\n",
    "from src.utils import Dict2Class, plot_training_results\n",
    "from src.cnn import CnnSentenceClassifier, CnnTextClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbcc76-19d3-464e-97ad-a972e4ea10e6",
   "metadata": {},
   "source": [
    "### Checking/Setting the Device\n",
    "\n",
    "PyTorch allows to train neural networks on supported GPU to significantly speed up the training process. If you have a support GPU, feel free to utilize it. However, for this notebook it's certainly not needed as our dataset is small and our network model is very simple. In fact, the training is fast on the CPU here since initializing memory on the GPU and moving the data to the GPU involves some overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f4cd4-66f7-48b8-b989-87399386266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Use this line below to enforce the use of the CPU (in case you don't have a supported GPU)\n",
    "# With this small dataset and simple model you won't see a difference anyway\n",
    "#use_cuda = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1021e-dbb7-4d8e-94db-5c37d0e0ea72",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66b14c-3c6f-4144-8d27-c691b612af86",
   "metadata": {},
   "source": [
    "### Dataset A: Sentence Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f088595-f1c3-4ec5-8dd4-cc1148772578",
   "metadata": {},
   "source": [
    "#### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b5a2e-7a13-4613-a369-290eca1c599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torch.load(\"data/datasets/sentence-polarities/polarity-corpus-10000.vocab\")\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(\"Size of vocabulary:\\t{}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1614053-7b39-484c-90af-8775d6d870d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, targets = [], []\n",
    "\n",
    "with open(\"data/datasets/sentence-polarities/polarity-dataset-vectors-10000.txt\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        # The input sequences and class labels are separated by a tab\n",
    "        sequence, label = line.split(\"\\t\")\n",
    "        # Convert sequence string to a list of integers (reflecting the indicies in the vocabulary)\n",
    "        sequence = [ int(idx) for idx in sequence.split()]\n",
    "        # Convert each sequence into a tensor\n",
    "        sequence = torch.LongTensor(sequence)\n",
    "        # Add sequence and label to the respective lists\n",
    "        sequences.append(sequence)\n",
    "        targets.append(int(label))\n",
    "        \n",
    "# As targets is just a list of class labels, we can directly convert it into a tensor\n",
    "targets = torch.LongTensor(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6e879-6234-4b45-914e-3a10e54b8c58",
   "metadata": {},
   "source": [
    "#### Create Training & Test Set\n",
    "\n",
    "To evaluate any classifier, we need to split our dataset into a training and a test set. With the method `train_test_split()` this is very easy to do; this method also shuffles the dataset by default, which is important for this example, since the dataset file is ordered with all positive sentences coming first. In the example below, we set the size of the test set to 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef731b-ee11-44c4-804c-2f223fc3f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "print(\"Number of training samples:\\t{}\".format(len(X_train)))\n",
    "print(\"Number of test samples:\\t\\t{}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cf792-ab1e-4c1a-b1f9-26061e79e04a",
   "metadata": {},
   "source": [
    "### Dataset B: IMDb Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af12bcb1-80bd-4318-bf4f-24fbcb2a3e8d",
   "metadata": {},
   "source": [
    "#### Load Data from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670f072-11d4-4d66-90f0-4553fcc4f465",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = torch.load(\"data/datasets/imdb-reviews/imdb-corpus-20000.vocab\")\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "print(\"Size of vocabulary:\\t{}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b759b3b-ce00-4750-9c04-fa9dd076d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train, samples_test = [], []\n",
    "\n",
    "with open(\"data/datasets/imdb-reviews/imdb-dataset-train-vectors-20000.txt\") as file:\n",
    "    for line in file:\n",
    "        name, label = line.split('\\t')\n",
    "        # Convert name to a sequence of integers\n",
    "        sequence = [ int(index) for index in name.split() ]\n",
    "        # Add (sequence,label) pair to list of samples\n",
    "        samples_train.append((sequence, int(label.strip())))\n",
    "        \n",
    "#with open(\"data/imdb/aclimdb-sentiment-test-vectorized-20000.txt\") as file:\n",
    "with open(\"data/datasets/imdb-reviews/imdb-dataset-test-vectors-20000.txt\") as file:    \n",
    "    for line in file:\n",
    "        name, label = line.split('\\t')\n",
    "        # Convert name to a sequence of integers\n",
    "        sequence = [ int(index) for index in name.split() ]\n",
    "        # Add (sequence,label) pair to list of samples\n",
    "        samples_test.append((sequence, int(label.strip())))\n",
    "        \n",
    "random.shuffle(samples_train)\n",
    "random.shuffle(samples_test)\n",
    "        \n",
    "print(\"Number of training samples: {}\".format(len(samples_train)))\n",
    "print(\"Number of test samples: {}\".format(len(samples_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3bca2-d6fb-4d1d-8294-e23cae26ef4c",
   "metadata": {},
   "source": [
    "#### Create Training & Test Set\n",
    "\n",
    "Since the dataset comes in 2 files reflecting the training and test data, we can directly convert the dataset into the respectice lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8096266-2736-4acc-a7c3-f62bec48f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [ torch.LongTensor(seq) for (seq, _) in samples_train ]\n",
    "X_test  = [ torch.LongTensor(seq) for (seq, _) in samples_test ]\n",
    "\n",
    "y_train = [ label for (_, label) in samples_train ]\n",
    "y_test  = [ label for (_, label) in samples_test ]\n",
    "\n",
    "# We can directly convert the vector of labels to a tensor\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test  = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccf913-3d54-43bc-a681-933436a7e7a6",
   "metadata": {},
   "source": [
    "### Create Dataset Class\n",
    "\n",
    "We first create a simple [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). This class only stores out `inputs` and `targets` and needs to implement the `__len__()` and `__getitem__()` methods. Since our class extends the abstract class [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), we can use an instance later to create a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Without going into too much detail, this approach does not only allow for cleaner code but also supports parallel processing on many CPUs, or on the GPU as well as to optimize data transfer between the CPU and GPU, which is critical when processing very large amounts of data. It is therefore the recommended best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c274ed-191f-4acf-aacc-7fa9dcea9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.targets is None:\n",
    "            return np.asarray(self.inputs[index])\n",
    "        else:\n",
    "            return np.asarray(self.inputs[index]), np.asarray(self.targets[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164f976-282d-4a14-881c-2df422be887e",
   "metadata": {},
   "source": [
    "### Create Data Loaders\n",
    "\n",
    "The [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class takes a `DataSet` object as input to handle to split the dataset into batches. The class `EqualLengthsBatchSampler` analyzes the input sequences to organize all sequences into groups of sequences of the same length. Then, each batch is sampled for a single group, ensuring that all sequences in the batch have the same length. In the following, we use a batch size of 256, although you can easily go higher since we are dealing with only sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dffa6e-2aa4-4516-b037-70a583f4b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "dataset_train = BaseDataset(X_train, y_train)\n",
    "sampler_train = EqualLengthsBatchSampler(batch_size, X_train, y_train)\n",
    "loader_train = DataLoader(dataset_train, batch_sampler=sampler_train, shuffle=False, drop_last=False)\n",
    "\n",
    "dataset_test = BaseDataset(X_test, y_test)\n",
    "sampler_test = EqualLengthsBatchSampler(batch_size, X_test, y_test)\n",
    "loader_test = DataLoader(dataset_test, batch_sampler=sampler_test, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada40ba5-55b7-43be-aca2-9364a20d99b5",
   "metadata": {},
   "source": [
    "## Train & Evaluate Model\n",
    "\n",
    "### Auxiliary Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36db0b-5763-4513-b423-4b3ab9ff2bd3",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "The code cell below implements the method `evaluate()` to, well, evaluate our model. Apart from the model itself, the method also receives the data loader as input parameter. This allows us later to use both `loader_train` and `loader_test` to evaluate the training and test loss using the same method.\n",
    "\n",
    "The method is very generic and is not specific to the dataset. It simply loops over all batches of the data loader, computes the log probabilities, uses these log probabilities to derive the predicted class labels, and compares the predictions with the ground truth to return the f1 score. This means, this method could be used \"as is\" or easily be adopted for all kinds of classifications tasks (incl. task with more than 2 classes).\n",
    "\n",
    "The method has 2 additional input parameters:\n",
    "\n",
    "* `fixed_seq_len`: Most CNN-based models assume inputs of a fixed size. We therefore need to specify this size so batches can be padded or cut accordingly. We have to set this parameter when using the class `CnnTextClassifier`.\n",
    "\n",
    "* `min_seq_len`: specifies the minimum size of a sequence; shorter sequences get padded up to this size. We need to ensure that a sequence is not shorter than the largest kernel, otherwise there will be an error. We only need this if the `fixed_length=None` and we use a CNN-model that uses 1-Max Pooling which ensures equal shapes for all batches. The class `CnnSentenceClassifier` uses 1-Max Pooling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d8973-b913-419b-8ed6-e73b04b2f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, fixed_seq_len=None, min_seq_len=None):\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "            \n",
    "            if fixed_seq_len is not None:\n",
    "                X_batch = create_fixed_length_batch(X_batch, fixed_seq_len)            \n",
    "                \n",
    "            if min_seq_len is not None:\n",
    "                X_batch = torch.nn.functional.pad(X_batch, (0, min_seq_len-seq_len), mode=\"constant\", value=0)         \n",
    "\n",
    "            # Move the batch to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            log_probs = model(X_batch)                \n",
    "\n",
    "            y_batch_pred = torch.argmax(log_probs, dim=1)\n",
    "\n",
    "            y_true += list(y_batch.cpu())\n",
    "            y_pred += list(y_batch_pred.cpu())\n",
    "            \n",
    "            pbar.update(batch_size)\n",
    "\n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f7af5-8254-4a4c-a970-c0325b33803c",
   "metadata": {},
   "source": [
    "### Train Model (single epoch)\n",
    "\n",
    "Similar to the method `evaluate()` we also implement a method `train_epoch()` to wrap all the required steps training. This has the advantage that we can simply call `train_epochs()` multiple times to proceed with the training. Apart from the model, this method has the following input parameters:\n",
    "\n",
    "* `optimizer`: the optimizer specifier how the computed gradients are used to updates the weights; in the lecture, we only covered the basic Stochastic Gradient Descent, but there are much more efficient alternatives available\n",
    "\n",
    "* `criterion`: this is the loss function; \"criterion\" is just very common terminology in the PyTorch documentation and tutorials\n",
    "\n",
    "The hear of the method is the snippet described as PyTorch Magic. It consists of the following 3 lines of code\n",
    "\n",
    "* `optimizer.zero_grad()`: After each training step for a batch if have to set the gradients back to zero for the next batch\n",
    "\n",
    "* `loss.backward()`: Calculating all gradients using backpropagation\n",
    "\n",
    "* `optimizer.step()`: Update all weights using the gradients and the method of the specific optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396f9ae-5af6-467a-9223-ffa7b44563f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, fixed_seq_len=None, min_seq_len=None):\n",
    "    \n",
    "    # Initialize epoch loss (cummulative loss fo all batchs)\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    with tqdm(total=len(loader)) as pbar:\n",
    "\n",
    "        for X_batch, y_batch in loader:\n",
    "            batch_size, seq_len = X_batch.shape[0], X_batch.shape[1]\n",
    "\n",
    "            if fixed_seq_len is not None:\n",
    "                X_batch = create_fixed_length_batch(X_batch, fixed_seq_len)            \n",
    "                \n",
    "            if min_seq_len is not None:\n",
    "                X_batch = torch.nn.functional.pad(X_batch, (0, min_seq_len-seq_len), mode=\"constant\", value=0)    \n",
    "            \n",
    "                \n",
    "            # Move the batch to the correct device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            log_probs = model(X_batch)                \n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(log_probs, y_batch)\n",
    "            \n",
    "            ### Pytorch magic! ###\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Keep track of overall epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            pbar.update(batch_size)\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1b176-2acb-4753-bf18-720d3caff0ae",
   "metadata": {},
   "source": [
    "#### Train Model (multiple epochs)\n",
    "\n",
    "The `train()` method combines the training and evaluation of a model epoch by epoch. The method keeps track of the loss, the training score, and the tests score for each epoch. This allows as later to plot the results; see below. Notice the calls of `model.train()` and `model.eval()` to set the models into the correct \"mode\". This is needed since our model contains a Dropout layer. For more details, check out this [Stackoverflow post](https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86bae9-9ef8-4cf8-b3ad-18566f5348ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader_train, loader_test, optimizer, criterion, num_epochs, fixed_seq_len=None, min_seq_len=None, verbose=False):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Total Training Time (total number of epochs: {})\".format(num_epochs))\n",
    "    #for epoch in tqdm(range(1, num_epochs+1)):\n",
    "    for epoch in range(1, num_epochs+1):        \n",
    "        model.train()\n",
    "        epoch_loss = train_epoch(model, loader_train, optimizer, criterion, fixed_seq_len=fixed_seq_len, min_seq_len=min_seq_len)\n",
    "        model.eval()\n",
    "        acc_train = evaluate(model, loader_train, fixed_seq_len=fixed_seq_len, min_seq_len=min_seq_len)\n",
    "        acc_test = evaluate(model, loader_test, fixed_seq_len=fixed_seq_len, min_seq_len=min_seq_len)\n",
    "\n",
    "        results.append((epoch_loss, acc_train, acc_test))\n",
    "        \n",
    "        if verbose is True:\n",
    "            print(\"[Epoch {}] loss:\\t{:.3f}, f1 train: {:.3f}, f1 test: {:.3f} \".format(epoch, epoch_loss, acc_train, acc_test))\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34458c13-748e-4e7a-85e8-af656c17d372",
   "metadata": {},
   "source": [
    "### Basic CNN Model\n",
    "\n",
    "We consider this model basic since the architecture is hard-coded and since the model uses 1-Max Pooling for convenience. As such, this model is arguably not suitable for long sequences, as 1-Max Pooling would throw away too much information. More specifically, implementing the architecture is presented in the lecture. However, we increase the size of the embeddings to 100 (although you can change that).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0dd647-d506-4123-95f2-bcda59fce5d3",
   "metadata": {},
   "source": [
    "<img src=\"data/images/CNN-modeling-on-text-Zhang-and-Wallace-2015.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f250c-a730-4d1c-9b0b-8b363148e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model   \n",
    "cnn = CnnSentenceClassifier(vocab_size, 2, 100).to(device)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "# Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba5cff-2a07-43f3-9ab2-aa8716e33066",
   "metadata": {},
   "source": [
    "#### Evaluate Untrained Model\n",
    "\n",
    "Let's first see how our model performs when untrain, i.e., with the initial random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b88e5-b624-4d81-880c-d0516db3f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cnn, loader_test, min_seq_len=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aa6997-5956-4f70-9b19-efa9cc03842a",
   "metadata": {},
   "source": [
    "### Full Training (and evaluation after each epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740d46c-e28a-4952-ba03-ca57af7070ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "#train(basic_rnn_classifier, loader, num_epochs, verbose=True)\n",
    "results = train(cnn, loader_train, loader_test, optimizer, criterion, num_epochs, min_seq_len=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67766399-fd0e-456d-a9f4-d8e72266cd87",
   "metadata": {},
   "source": [
    "In `src.utils` you can find the method `plot_training_results()` to plot the losses and accuracies (training + test) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f803c7d-c4b1-4567-b940-471d1e26ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79768d97-54cf-40d8-8c16-895141ad18ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63250442-d2d1-4bc5-a572-6558389aef90",
   "metadata": {},
   "source": [
    "## Advanced CNN Text Classifier\n",
    "\n",
    "The more advanced CNN-based Classifier uses normal Max Pooling instead of 1-Max Pooling. As such, we need to ensure that each sequences is indeed of the same length. The method below takes a batch of sequences and pad batches that are too short and cuts batches that are too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fd494-fa47-4aff-abe9-402d9cd98a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fixed_length_batch(sequences, length):\n",
    "    \n",
    "    # Pad sequences w.r.t. longest sequences\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences,  batch_first=True, padding_value=0)\n",
    "\n",
    "    # Get the current sequence length\n",
    "    max_seq_len = sequences_padded.shape[1]\n",
    "    \n",
    "    if max_seq_len > length:\n",
    "        # Cut sequences if to0 long\n",
    "        return sequences_padded[:,:length]\n",
    "    else:\n",
    "        # Pad sequences if too short\n",
    "        return torch.nn.functional.pad(sequences_padded, (0, length-max_seq_len), mode=\"constant\", value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e7f6b-466c-4954-8b13-946242cc456d",
   "metadata": {},
   "source": [
    "The classifier is also more flexible in the sense that it allows to specify a whole range of parameters such as the size of the kernels, the number of output channels, the Max Pooling parameters and so on. It also allows you to customize the number of hidden layers that map the output of the convolutional and pooling layers to the output layer. Feel free to have a look at the implementation of class `CnnTextClassifier`. It looks quite verbose but most of the code is needed to allow for the flexibility compared to hard-coding the number of layers and their sizes (or other parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98df5c-5745-477b-9b54-9c7ef98d84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_LENGTH = 100\n",
    "\n",
    "params = {\n",
    "    \"seq_len\": FIXED_LENGTH,\n",
    "    \"in_channels\": 1,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"embed_size\": 300,\n",
    "    \"conv_kernel_sizes\": [2,3,4],\n",
    "    \"out_channels\": 10,\n",
    "    \"conv_stride\": 1,\n",
    "    \"conv_padding\": 1,\n",
    "    \"maxpool_kernel_size\": 2,\n",
    "    \"maxpool_padding\": 0,\n",
    "    \"linear_sizes\": [64],\n",
    "    \"linear_dropout\": 0.5,\n",
    "    \"output_size\": 2\n",
    "}\n",
    "\n",
    "# Define model paramaters\n",
    "params = Dict2Class(params)\n",
    "# Create model  \n",
    "cnn = CnnTextClassifier(params).to(device)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "# Define loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62533ce2-8f44-417c-8bab-ca6a4421ca29",
   "metadata": {},
   "source": [
    "#### Set Pretrained Word Embeddings (optional)\n",
    "\n",
    "If we want to use pre-trained word embeddings, e.g., Word2Vec, this is the moment to do. A source for pre-trained word embeddings is [this site](http://vectors.nlpl.eu/repository/). When downloading the a file containing pre-trained word embeddings, there are some things to consider:\n",
    "\n",
    "* Most obviously, the pre-trained embeddings should match the language (here: English).\n",
    "\n",
    "* The pretrained embeddings should match the preprocessing steps. For example, we lemmatized our dataset for this notebook (at least by default, maybe you have changed it). So we need embeddings trained over a lemmatized dataset as well.\n",
    "\n",
    "* The pretrained embeddings have to match the size of our embedding layer. So if we create a embedding layer of size 300, we have to use pretrained embeddings of the same size\n",
    "\n",
    "* The files with the pretrained embeddings are too large to ship with the notebooks, so you have to download them separately :)\n",
    "\n",
    "First, we need to load the pretrained embeddings from the file; here I used [this file](http://vectors.nlpl.eu/repository/20/5.zip) (lemmatized, size: 300):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b3a30-700a-4061-8280-19de5351bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vectors = torchtext.vocab.Vectors(\"data/embeddings/model.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e016860-da6c-4f8f-9595-2c5477296dd9",
   "metadata": {},
   "source": [
    "Now we have over 270k pretrained word embeddings, but we only have 20k words in our vocabulary. So we need to create an embedding -- which is basically just a $20k \\times 300$ matrix containing the respective 20k pretrained word embeddings for our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664b36e-762a-4f66-9e48-32ad01bd74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = pretrained_vectors.get_vecs_by_tokens(vocabulary.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2fec37-48ae-4be9-b806-5c5d00ced3ec",
   "metadata": {},
   "source": [
    "Now we can set the weights of the embedding layer of our model to the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee6444-ca69-4230-be80-ab11c904822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.embedding.weight.data = pretrained_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e83df-e677-40d4-aaf5-13cca80fe365",
   "metadata": {},
   "source": [
    "Lastly, we can decide if we want the pretrained embeddings to remain fixed or whether we want to update them during training. By setting `.requires_grad = False`, we tell the optimizer to \"freeze\" the layer **not** to update the embedding weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a1265-cba2-4fcf-83a0-721f33634fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ed4c3-d1fe-43fe-9a01-49432fabc3e2",
   "metadata": {},
   "source": [
    "Since the embedding weights still reside on the CPU, we can move the model to the respective device so that the model on all data is indeed on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23265e7f-7d00-46ed-a7c1-2d382e50915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f23999-8b16-4d47-933c-6b6aed77c129",
   "metadata": {},
   "source": [
    "#### Evaluate Untrained Model\n",
    "\n",
    "Let's first see how our model performs when untrain, i.e., with the initial random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d1400-85eb-4aaa-b47c-ba524080e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cnn, loader_test, fixed_seq_len=FIXED_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c28a47-05b3-4007-a63d-057d7c5b9d70",
   "metadata": {},
   "source": [
    "### Full Training (and evaluation after each epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc3292-5c1e-4c9a-b4c6-400815bfcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "#train(basic_rnn_classifier, loader, num_epochs, verbose=True)\n",
    "results = train(cnn, loader_train, loader_test, optimizer, criterion, num_epochs, fixed_seq_len=FIXED_LENGTH, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585923b-8b88-4aeb-b206-ddc1efed487c",
   "metadata": {},
   "source": [
    "In `src.utils` you can find the method `plot_training_results()` to plot the losses and accuracies (training + test) over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f618a73-b73b-454a-b780-d62864076edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251272c-26f7-4adf-9868-f03be15d7161",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc63f66-3cd0-4282-80e1-f31e304b6f6c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Convolutional Neural Networks (CNNs), primarily known for their excellence in image processing, have found a remarkable application in text classification tasks like sentiment analysis. When adapted to process text data, CNNs leverage their ability to detect local patterns and hierarchies of features, proving effective in analyzing sequences of words.\n",
    "\n",
    "In text classification, CNNs operate by treating text as a 1-dimensional sequence, where each word or character corresponds to a position in the sequence. Convolutional layers slide filters or kernels across these sequences, detecting local features or patterns in groups of words. These filters capture information such as word combinations or phrases that are indicative of specific sentiments or categories.\n",
    "\n",
    "CNNs' strength lies in their capability to learn hierarchical representations of text. Lower-level filters might detect simple patterns like word sequences or n-grams, while higher-level filters capture more complex structures. Pooling layers then downsample these features, extracting the most relevant information while reducing dimensionality.\n",
    "\n",
    "One of the advantages of CNNs for text classification tasks is their ability to learn from local context, identifying essential phrases or combinations of words without being sensitive to the order of the entire sequence. This makes them effective for tasks where local context matters more than the global sequence structure, as in sentiment analysis. Moreover, CNNs exhibit computational efficiency and can process text in parallel, making them suitable for handling large-scale text data efficiently. Overall, their capacity to discern intricate patterns within text data and extract relevant features has made CNNs a powerful and competitive choice for various text classification tasks, including sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db2446-999f-4ec8-8d0e-6ed9bd8c06ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
