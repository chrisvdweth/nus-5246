{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219a5367-a3dc-4acc-8a51-554aa522675b",
   "metadata": {},
   "source": [
    "<img src=\"data/images/lecture-notebook-header.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35cec76-721f-4227-ae55-14bae6046380",
   "metadata": {},
   "source": [
    "# Sentiment Analysis -- Data Preparation\n",
    "\n",
    "When it comes to machine learning with text data, it's often a good idea to treat the transformation for the raw input corpus to the training and test set as valid input for the neural network as a separate step. This is particularly true of the size of the corpus is huge. One of the datasets we consider consists of 50,000 movie reviews, annotated with positive or negative labels. While this dataset is far from huge, you will notices that it will takes some time to preprocess.\n",
    "\n",
    "Additionally, since already preprocessing requires making certain design choices (e.g., the consideration of the most frequent words, the performing of stemming/lemmatization, or stopword removal, etc.), creating several conversions of the text documents can often be meaningful in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fed3e7-ae7f-4309-8f3d-63f89c99d5ce",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e942f-09a3-4ce1-a674-9217da08b707",
   "metadata": {},
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3c574-bf84-4163-b5ae-7578f23d80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2cfa3-f572-4410-a63c-45980a51e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8546d29-a6a9-4525-a898-c3e34296f7ac",
   "metadata": {},
   "source": [
    "Lastly, `src/utils.py` provides some utility methods to download and decompress files. Since the datasets used in some of the notebooks are of considerable size -- although far from huge -- they are not part of the repository and need to be downloaded (and optionally decompressed) separately. The 2 methods `download_file` and `decompress_file` accomplish this for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c573a9-dfb0-49fe-8ddf-2f150ea8ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import download_file, decompress_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e4d21-5d3d-43af-9ce1-8cdb715e7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "# We use spaCy for preprocessing, but we only need the tokenizer and lemmatizer\n",
    "# (for a large real-world dataset that would help with the performance)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1970e3-8a10-4fa3-b11f-aecd2f919fb7",
   "metadata": {},
   "source": [
    "We consider 2 dataset for sentiment analysis (binary classifications) of different size, although even the larger dataset is still rather small:\n",
    "* 10k sentences with a positive or negative sentiment (balanced)\n",
    "* 50k multisentence movie reviews with a positive or negative (balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1312cd4-53c6-4f4a-99d5-ae9cca05d03a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312cf71-be9e-4101-b802-1366664e708f",
   "metadata": {},
   "source": [
    "## Sentence Polarity\n",
    "\n",
    "The [sentence polarity dataset](https://www.kaggle.com/datasets/nltkdata/sentence-polarity) is a well-known dataset commonly used for sentiment analysis and text classification tasks in NLP. It consists of sentences or short texts labeled with their corresponding sentiment polarity (positive or negative). This dataset is often used to train and evaluate models that aim to classify text into positive or negative sentiment categories. It serves as a benchmark for sentiment analysis tasks and provides a standardized dataset for researchers and practitioners to compare and evaluate the performance of different algorithms and techniques.\n",
    "\n",
    "There are several versions and variations of the sentence polarity dataset available, created for different purposes and domains. One of the popular versions is the Movie Review Dataset, also known as the Pang and Lee dataset, created by Bo Pang and Lillian Lee. This dataset contains movie reviews from the website IMDb, with each review labeled as positive or negative. The sentence polarity dataset enables researchers and developers to build and test sentiment analysis models that can automatically determine the sentiment expressed in text, allowing applications such as sentiment monitoring, opinion mining, and customer feedback analysis.\n",
    "\n",
    "For this notebook, we already prepared the dataset by combining the 2 files containing the positive and negative sentences into a single file. The polarity of each sentence is denoted by a polarity label: `1` for positive and `-1` for negative. This makes handling the data a bit simpler and keeps the notebook a bot cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fab351-d8ac-4ffd-857d-7e8d9c7e13ad",
   "metadata": {},
   "source": [
    "#### Auxiliary Method\n",
    "\n",
    "The method `preprocess()`, well, tokenizes a given text. In this case, we not only tokenize but also lemmatize and lowercase all tokens. The exact list of preprocessing steps will in practice depend on the exact task, but this is what we do here. Notice that we do not, for example, remove stopwords. This is mainly to reduce the vocabulary size not too much here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828614e-7db9-41d0-8311-40c8973005b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return [token.lemma_.lower() for token in nlp(text)]\n",
    "\n",
    "preprocess(\"This is a test to see if the TOKENIZER does its job.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcc807-d3e8-4ead-94f6-0607e4f96f6f",
   "metadata": {},
   "source": [
    "#### Read Files & Compute Word Frequencies\n",
    "\n",
    "The first to go through the whole corpus and count the number of occurrences for each token. 10k sentences is basically nothing these days, but the purpose of this notebook is not to focus on large scale data as the steps would be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7be3f-c34c-4dc3-9ee3-c245910eb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "\n",
    "targets_polarity = []\n",
    "\n",
    "with tqdm(total=10662) as pbar:\n",
    "    \n",
    "    # Loop over each sentence (1 sentence per line)\n",
    "    with open('data/datasets/sentence-polarities/sentence-polarities.csv', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            parts = line.split('\\t')\n",
    "            sentence, label = parts[0], int(parts[1])\n",
    "            # Update token counts\n",
    "            for token in preprocess(sentence):\n",
    "                token_counter[token] += 1            \n",
    "            # Add label to targets list\n",
    "            targets_polarity.append(label)\n",
    "            # Update progress bar\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f4806-ec6f-4601-95cc-95f4b6fc6d0c",
   "metadata": {},
   "source": [
    "#### Create Vocabulary\n",
    "\n",
    "To create our `vocab` object, we perform exactly the same steps as above. The only difference is that our \"full\" vocabulary is not larger (although with less than 20k tokens still rather small). We therefore limit the vocabulary here to the 10,000 most frequent tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba958a-204b-46a8-8baf-39711c233be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by word frequency\n",
    "token_counter_sorted = sorted(token_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(token_counter_sorted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aab979-1afa-4852-bcd1-35dd366821cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TOKENS = 10000\n",
    "\n",
    "token_counter_sorted = token_counter_sorted[:TOP_TOKENS]\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(token_counter_sorted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840adc9-1a3d-4154-b65e-bbdae59c1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ordered_dict = OrderedDict(token_counter_sorted)\n",
    "\n",
    "# Define list of \"special\" tokens\n",
    "SPECIALS = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "vocabulary = vocab(token_ordered_dict, specials=SPECIALS)\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[\"<UNK>\"])\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0eae2c-4e42-4a05-953a-8de2eec2a1f7",
   "metadata": {},
   "source": [
    "### Save Dataset\n",
    "\n",
    "Lastly, we save all the data for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54197206-2a8a-4ee1-a7c3-69d8702c89cd",
   "metadata": {},
   "source": [
    "#### Vectorize and Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192c849-ef39-43cd-9078-6a53c512920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"data/datasets/sentence-polarities/polarity-dataset-vectors-{}.txt\".format(TOP_TOKENS), \"w\")\n",
    "\n",
    "with tqdm(total=10662) as pbar:\n",
    "    \n",
    "    # Loop over each sentence (1 sentence per line)\n",
    "    with open('data/datasets/sentence-polarities/sentence-polarities.csv', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            parts = line.split('\\t')\n",
    "            sentence, label = parts[0], int(parts[1])\n",
    "            # Convert labels from -1/1 to 0/1\n",
    "            label = int((label + 1) / 2)\n",
    "            # Convert sentence into sequence of word indices\n",
    "            vector = vocabulary.lookup_indices(preprocess(sentence))\n",
    "            # Write converted sequence and labelsto file\n",
    "            output_file.write(\"{}\\t{}\\n\".format(\" \".join([str(idx) for idx in vector]), label))\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "output_file.flush()\n",
    "output_file.close()            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2a371-1779-402e-a0a7-fdbce7697e6f",
   "metadata": {},
   "source": [
    "#### Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcdd38-af96-4884-a79f-1479b6d0f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file_name = \"data/datasets/sentence-polarities/polarity-corpus-{}.vocab\".format(TOP_TOKENS)\n",
    "\n",
    "torch.save(vocabulary, vocabulary_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e35b9-23fb-413d-ae96-9c2356c44ac7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff74af-ca1d-420a-8916-b4bc9035d3e3",
   "metadata": {},
   "source": [
    "## IMDb Movie Reviews\n",
    "\n",
    "The [Large Movie Review Datase](https://ai.stanford.edu/~amaas/data/sentiment/), commonly known as the IMDb dataset or IMDb movie reviews dataset, is a widely used benchmark dataset in natural language processing (NLP) and sentiment analysis. Created by Andrew Maas and a group of researchers at Stanford University, this dataset consists of movie reviews collected from IMDb (Internet Movie Database).\n",
    "\n",
    "Here are the key characteristics of the Large Movie Review Dataset:\n",
    "\n",
    "* **Data Size:** It contains a collection of 50,000 movie reviews.\n",
    "\n",
    "* **Review Split:** The dataset is evenly divided into two sets:\n",
    "    * 25,000 reviews for training\n",
    "    * 25,000 reviews for testing\n",
    "\n",
    "* **Sentiment** Labels: Each review is labeled with sentiment polarity:\n",
    "    * 50% of reviews are labeled as positive\n",
    "    * 50% of reviews are labeled as negative\n",
    "\n",
    "* **Binary Classification Task:** The dataset is commonly used for binary sentiment classification tasks, where the goal is to classify whether a review expresses positive or negative sentiment.\n",
    "\n",
    "This dataset serves as a standard benchmark for sentiment analysis and text classification algorithms, enabling researchers and developers to evaluate and compare the performance of different machine learning and deep learning models in sentiment classification tasks. The availability of labeled data in large quantities allows for the training and evaluation of models to predict sentiment accurately, making it a valuable resource in the field of natural language processing and sentiment analysis research.\n",
    "\n",
    "Given its size, the dataset is not included in the Github repository. You can either download the dataset yourself using the link above, or you can run the notebook \"Representations (Word2Vec - Data Preparation)\" first which downloads the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71464cfa-d831-4a5e-a7a2-67d3e97bbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_train = [\n",
    "    'data/datasets/imdb-reviews/aclImdb/train/pos',\n",
    "    'data/datasets/imdb-reviews/aclImdb/train/neg'  \n",
    "]\n",
    "\n",
    "folders_test = [\n",
    "    'data/datasets/imdb-reviews/aclImdb/test/pos',\n",
    "    'data/datasets/imdb-reviews/aclImdb/test/neg'  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62622fd4-c8c5-42b9-baaa-996f2b52adc0",
   "metadata": {},
   "source": [
    "### Auxiliary Method for Data Cleaning & Preprocessing\n",
    "\n",
    "The method below takes a single review file as input and returns all valid tokens as a list. This includes that the method removes all punctuation marks and stopwords. The method performs lemmatization. Recall from the lecture how preprocessing affects the learning of word embeddings but here we want to keep it simple and try to minimize the vocabulary, i.e., the number of unique tokens.\n",
    "\n",
    "Since the movie reviews can include HTML tags, we remove those as well using RegEx. Again, anything here is kept to a bare minimum to keep things short and simple. Feel free to put in more thoughts into potentially better preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a81b3-f50b-4134-923d-63da0042c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_name):\n",
    "    text = None\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().replace('\\n', '')\n",
    "        \n",
    "    if text is None:\n",
    "        return\n",
    "\n",
    "    ## Remove HTML tags\n",
    "    p = re.compile(r'<.*?>')\n",
    "    text = p.sub(' ', text)\n",
    "    \n",
    "    ## Let spaCy do its magic\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    ## Return \"proper tokens\" (lemmatize, lowercase)\n",
    "    ##return [ t.lemma_.lower() for t in doc if t.pos_ not in ['PUNCT'] and t.dep_ not in ['punct'] and t.lemma_.strip() != '' and t.is_stop == False ]\n",
    "    return [ t.lemma_.lower() for t in doc if t.pos_ not in ['PUNCT'] and t.dep_ not in ['punct'] and t.lemma_.strip() != '']\n",
    "\n",
    "process_file(\"data/datasets/imdb-reviews/aclImdb/train/pos/0_9.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ddd68b-20e4-44a4-8661-4be2afab65de",
   "metadata": {},
   "source": [
    "### Process Review Files\n",
    "\n",
    "The code cell below iterates over all text files representing the movie reviews in the specified folders, see above. For each review, we first extract all the tokens using `process_file()`. This returns the list of relevant tokens for this review which append to a list of all tokens across all reviews.\n",
    "\n",
    "For each token, we also keep track of its count. We only need this to later create the final vocabulary by only looking at the top-k (e.g., top-20k most frequent) words.\n",
    "\n",
    "For testing, it's recommended to use a lower value for `num_reviews` (e.g., 1000) to see if this and the other notebooks are working (of course, the results won't be great). Once you think all is good, you can set `num_reviews` to infinity to work on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7a492-c650-4707-b03e-0fcaa0f095ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the number of reviews taken from each folder\n",
    "num_reviews = 1000000000000\n",
    "\n",
    "token_counter = Counter()\n",
    "    \n",
    "## Loop through all folders and files\n",
    "for folder in folders_train:\n",
    "\n",
    "    ## Get all filen names and limit as specified\n",
    "    file_names = sorted(glob.glob('{}/*.txt'.format(folder)))[:num_reviews]\n",
    "    \n",
    "    with tqdm(total=len(file_names)) as t:\n",
    "        ## Loop over each file (1 file = 1 review)\n",
    "        for file_name in file_names:\n",
    "            ## Extract tokens from file/review\n",
    "            tokens = process_file(file_name)\n",
    "            ## Update token counter\n",
    "            for token in tokens:\n",
    "                token_counter[token] += 1\n",
    "            ## Update progress bar\n",
    "            t.update(1)\n",
    "\n",
    "            \n",
    "print('Size of Vocabulary: {}'.format(len(token_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a70e00-2200-4af2-a2de-ff0d774b3d2e",
   "metadata": {},
   "source": [
    "#### Create Vocabulary\n",
    "\n",
    "To create our `vocab` object, we perform exactly the same steps as above. The only difference is that our \"full\" vocabulary is not larger. We therefore limit the vocabulary here to the 20,000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a66d4-5084-451e-a438-152262071841",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TOKENS = 20000\n",
    "\n",
    "# Sort with respect to frequencies\n",
    "token_counter_sorted = sorted(token_counter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "token_ordered_dict = OrderedDict(token_counter_sorted[:TOP_TOKENS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931fe98-74de-4423-a0be-18684f49b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "SPECIALS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "\n",
    "vocabulary = vocab(token_ordered_dict, specials=SPECIALS)\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[UNK_TOKEN])\n",
    "\n",
    "print(\"Number of tokens: {}\".format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d48e1-75b6-4f2b-8ba1-fe6bba7dacbe",
   "metadata": {},
   "source": [
    "### Save Dataset\n",
    "\n",
    "Lastly, we again save all the data for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38b64b-638e-47ff-9a38-c8648869214c",
   "metadata": {},
   "source": [
    "#### Vectorize and Save Dataset\n",
    "\n",
    "To preserve the split between the original training and test data, we save the data in 2 separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f3d22-419b-43aa-822a-94160f1ac3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"data/datasets/imdb-reviews/imdb-dataset-train-vectors-{}.txt\".format(TOP_TOKENS), \"w\")\n",
    "\n",
    "## Loop through all folders and files (1 file = 1 review)\n",
    "for label, folder in enumerate(folders_train):\n",
    "\n",
    "    ## Get all filen names and limit as specified\n",
    "    file_names = sorted(glob.glob('{}/*.txt'.format(folder)))[:num_reviews]\n",
    "    \n",
    "    with tqdm(total=len(file_names)) as t:\n",
    "        ## Loop over each file (1 file = 1 review)\n",
    "        for file_name in file_names:\n",
    "            ## Extract tokens from file/review\n",
    "            tokens = process_file(file_name)\n",
    "            vector = vocabulary.lookup_indices(tokens)\n",
    "            # Write both texts to the output file (use tab as separator)\n",
    "            output_file.write(\"{}\\t{}\\n\".format(\" \".join([str(idx) for idx in vector]), label))\n",
    "            ## Update progress bar\n",
    "            t.update(1)            \n",
    "        \n",
    "output_file.flush()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7cd801-10f0-4c61-b13a-a1400b3d5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"data/datasets/imdb-reviews/imdb-dataset-test-vectors-{}.txt\".format(TOP_TOKENS), \"w\")\n",
    "\n",
    "## Loop through all folders and files (1 file = 1 review)\n",
    "for label, folder in enumerate(folders_test):\n",
    "\n",
    "    ## Get all filen names and limit as specified\n",
    "    file_names = sorted(glob.glob('{}/*.txt'.format(folder)))[:num_reviews]\n",
    "    \n",
    "    with tqdm(total=len(file_names)) as t:\n",
    "        ## Loop over each file (1 file = 1 review)\n",
    "        for file_name in file_names:\n",
    "            ## Extract tokens from file/review\n",
    "            tokens = process_file(file_name)\n",
    "            vector = vocabulary.lookup_indices(tokens)\n",
    "            # Write both texts to the output file (use tab as separator)\n",
    "            output_file.write(\"{}\\t{}\\n\".format(\" \".join([str(idx) for idx in vector]), label))\n",
    "            ## Update progress bar\n",
    "            t.update(1)            \n",
    "        \n",
    "output_file.flush()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd578a-fe55-4a79-bd78-7cd76c196853",
   "metadata": {},
   "source": [
    "#### Save Metadata\n",
    "\n",
    "We only need to save the vocabulary since the class labels are already 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece394f1-d567-4d56-a4c9-adcbd820b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_file_name = \"data/datasets/imdb-reviews/imdb-corpus-{}.vocab\".format(TOP_TOKENS)\n",
    "\n",
    "torch.save(vocabulary, vocabulary_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8777590-3f8b-4d5c-b396-b9d2efba97ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ce420-5692-471b-afda-716eeb3b4128",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings\n",
    "\n",
    "Word embeddings are numerical representations of words in a continuous vector space. Pretrained word embeddings are vector representations of words that are derived from large corpora of text using unsupervised learning techniques. These embeddings capture semantic and syntactic information about words in a dense vector space, where words with similar meanings or contexts are located closer to each other.\n",
    "\n",
    "Once trained, these word embeddings can be reused in various downstream natural language processing (NLP) tasks, such as text classification, named entity recognition, *sentiment analysis*, and machine translation. By utilizing pretrained word embeddings, models can leverage the learned semantic relationships between words and benefit from transfer learning. Pretrained word embeddings have become popular because they offer several advantages. First, they capture rich semantic information that might be challenging to learn from smaller task-specific datasets. Second, they can help overcome the data sparsity problem, especially when dealing with rare words or out-of-vocabulary (OOV) terms. Lastly, pretrained word embeddings enable faster convergence and improved generalization for downstream NLP tasks.\n",
    "\n",
    "Examples of popular pretrained word embeddings include *Word2Vec*, GloVe (Global Vectors for Word Representation), and fastText. These embeddings are typically available in prebuilt formats and can be readily loaded into models to enhance their performance on various NLP tasks. In later notebooks, we will actually train Word2Vec embeddings from scratch.\n",
    "\n",
    "The notebook introducing and implementing an RNN-based model for sentiment analysis includes an optional step to utilize pretrained word embeddings. Such embeddings based on Word2Vec, GloVe, or fastText are available only for download. For example [http://vectors.nlpl.eu/repository/](http://vectors.nlpl.eu/repository/) is an online repository for pretrained word embeddings. In the code cells below, we download and decompress the ZIP file containing the embeddings used for training the sentiment analysis model. Note that these word embeddings have been trained on a lemmatized text corpus. This matches -- and has to match -- the preprocessing steps of the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292370e-d860-454b-93ab-c235a45092da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Download file...')\n",
    "download_file('http://vectors.nlpl.eu/repository/20/5.zip', 'data/embeddings/')\n",
    "print('Decompress file...')\n",
    "decompress_file('data/embeddings/5.zip', 'data/embeddings/')\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1869810-3f2d-457d-96bd-576b3a965c8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d9b9e-fb10-43b9-b304-c72d1c27593c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "While we didn't do anything exciting here, this notebook has a couple of useful take-away messages:\n",
    "\n",
    "* For large(r) text corpora it is a good practice to consider the preprocessing and preparation of the final dataset (incl. vectorization) as a separate step that requires a lot of consideration, and can be very time and resource-intensive on its own without any training of neural network models. In the follow-up notebooks, we will utilize the dataset generated in this notebook.\n",
    "\n",
    "* Even when using the same corpus, different tasks are likely to require different preprocessing steps. For example, one of the main differences in this notebook was that we lemmatized the data for sentiment analysis (arguably debateable) but not for training language models (arguably mandatory).\n",
    "\n",
    "* The preprocessing and vectorization of text corpora generally involves the same steps. Utilizing and benefitting from well-established packages such as `torchtext` is very recommended. The provided methods are mostly very flexible. Only in the case of very non-standard preprocessing and vectorization steps, any custom implementations should be required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcf5b4-cc21-479f-8e96-dd199228c761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs5246]",
   "language": "python",
   "name": "conda-env-cs5246-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
